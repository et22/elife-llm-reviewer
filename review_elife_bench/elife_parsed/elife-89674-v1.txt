TITLE
A neuronal least-action principle for real-time learning in cortical circuits

ABSTRACT
One of the most fundamental laws of physics is the principle of least action. Motivated by its predictive power, we introduce a neuronal least-action principle for cortical processing of sensory streams to produce appropriate behavioral outputs in real time. The principle postulates that the voltage dynamics of cortical pyramidal neurons prospectively minimizes the local somato-dendritic mismatch error within individual neurons. For output neurons, the principle implies minimizing an instantaneous behavioral error. For deep network neurons, it implies the prospective firing to overcome integration delays and correct for possible output errors right in time. The neuron-specific errors are extracted in the apical dendrites of pyramidal neurons through a cortical microcircuit that tries to explain away the feedback from the periphery, and correct the trajectory on the fly. Any motor output is in a moving equilibrium with the sensory input and the motor feedback during the ongoing sensory-motor transform. Online synaptic plasticity reduces the somatodendritic mismatch error within each cortical neuron and performs gradient descent on the output cost at any moment in time. The neuronal least-action principle offers an axiomatic framework to derive local neuronal and synaptic laws for global real-time computation and learning in the brain.

INTRODUCTION
Wignerâ€™s remark about the â€˜unreasonable effectivenessâ€™ of mathematics in allowing us to understand physical phenomena Wigner, 1960 is famously contrasted by Gelfandâ€™s quip about its â€˜unreasonable ineffectivenessâ€™ in doing the same for biology (Borovik, 2021). Considering the component of randomness that is inherent to evolution, this may not be all that surprising. However, while this argument holds just as well for the brain at the cellular level, ultimately brains are computing devices. At the level of computation, machine learning, and neuroscience have revealed near-optimal strategies for information processing and storage, and evolution is likely to have found similar principles through trial and error (Hassabis et al., 2017). Thus, we have reason to hope for the existence of fundamental principles of cortical computation that are similar to those we have found in the physical sciences. Eventually, it is important for such approaches to relate these principles back to brain phenomenology and connect function to structure and dynamics.
In physics, a fundamental measure of â€˜effortâ€™ is the action of a system, which nature seeks to â€˜minimize.â€™ Given an appropriate description of interactions between the systemâ€™s constituents, the least-action principle can be used to derive the equations of motion of any physical system (Feynman et al., 2011; Coopersmith, 2017). Here, we suggest that in biological information processing, a similar principle holds for prediction errors, which are of obvious relevance for cognition and behavior.
Based on such errors, we formulate a neuronal least-action (NLA) principle which can be used to derive neuronal dynamics and map them to observed dendritic morphologies and cortical microcircuits. Within this framework, local synaptic plasticity at basal and apical dendrites can be derived by stochastic gradient descent on errors. The errors that are minimized refer to the errors in output neurons that are typically thought to represent motor trajectories, planned and encoded in cortical motor areas and ultimately in the spinal cord and muscles. In the context of motor control, a phenomenological â€˜minimal action principleâ€™ has previously been proposed that guides the planning and execution of movements (Feldman and Levin, 2009). Our neuronal least-action principle reformulates and formalizes the classical equilibrium point hypothesis (Latash, 2010) in a dynamical setting, linking it to optimality principles in sensory-motor control (Todorov, 2004).
Other attempts exist to link biological information processing and neural networks with the least-action principle, for instance by directly learning to reproduce a given trajectory (Amirikian and Lukashin, 1992), by minimizing the physical action for the muscle force generation by motor unit recruitment (Senn et al., 1995), minimizing cognitive prediction errors (Alonso et al., 2012), minimizing output errors with a weight-change regularization (Betti and Gori, 2016), minimizing psychomotor work (Fox and Kotelba, 2018), minimizing data transport through a network (Karkar et al., 2021), minimizing the discrimination information (Summers, 2021), or minimizing the free energy (Friston, 2010; Friston et al., 2022). Apart from the latter, however, these attempts remain far from the biology that seems to resist a formalization with the tool of physics â€“ at least, when applied too strictly.
The fundamental novelty of our NLA principle is the way it deals with time. In physics, bodies interact based on where they are now, irrespective of what happens in the future. Living systems, instead, interact based on what could happen in the near future, and react early to stay alive. This difference is also mirrored in the way our NLA principle looks for an error-minimizing trajectory of brain states. We postulate that the brain trades with near-future states and seeks for a path that minimizes errors of these future states at any moment in time. Looking ahead towards what will likely happen allows the network for correcting the internal trajectory of deep neurons early enough so that the delayed output moves along the desired path. The notion of looking into the future to gate a dynamical system is also central in optimal control theory (as expressed by the Bellman equation, see e.g. Todorov, 2006). Yet, starting with a neuronal action is more principled as it includes the derivation of the dynamical system itself that will be optimally controlled.
The insight into the time structure of biological information processing allows us to express a simple form of a total â€˜mismatch energyâ€™ for our cortical neuronal networks, from which we derive the dynamic neuronal and synaptic laws.In short, the mismatch energy within a single pyramidal neuron is the squared prediction error between basal dendrites and the soma, together with the apical dendrites receiving a top-down feedback. The apical dendrites calculate a local prospective prediction error that looks ahead in time and overcomes neuronal integration delays (Figure 1a). As a consequence, the output neurons are corrected on the fly by the prospective error processing, pushing them in real time closer to the desired path. In addition, the prospective errors are suited for gradient learning of the sensory synapses on the basal dendrites. This gradient learning is proven to reduce the error in the output neurons at any moment in time.
Figure 1: Somato-dendritic mismatch energies and the neuronal least-action (NLA) principle. (a1) Sketch of a cross-cortical network of pyramidal neurons described by NLA. (a2) Correspondence between elements of NLA and biological observables such as membrane voltages and synaptic weights. (b1) The NLA principle postulates that small variations Î´ğ’–~ (dashed) of the trajectories ğ’–~ (solid) leave the action invariant, Î´A=0. It is formulated in the look-ahead coordinates ğ’–~ (symbolized by the spyglass) in which `hills' of the Lagrangian (shaded gray zones) are foreseen by the prospective voltage so that the trajectory can turn by early enough to surround them. (b2) In the absence of output nudging (Î²=0), the trajectory ğ’–(t) is solely driven by the sensory input, and prediction errors and energies vanish (L=0, outer blue trajectory at bottom). When nudging the output neurons towards a target voltage (Î²&gt;0), somatodendritic prediction errors appear, the energy increases (red dashed arrows symbolising the growing â€˜volcanoâ€™) and the trajectory ğ’–(t) moves out of the L=0 hyperplanes, riding on top of the `volcano' (red trajectory). Synaptic plasticity WË™ reduces the somatodendritic mismatch along the trajectory by optimally â€˜shoveling down the volcanoâ€™ (blue dashed arrows) while the trajectory settles in a new place on the L=0 hyperplane (inner blue trajectory at bottom).
The NLA principle builds on and integrates various ingredients from existing work and theories. Output neurons, be they motor neurons or decision-making neurons, are postulated to be â€˜nudgedâ€™ towards the desired target time course by additional synaptic input to the soma or the proximal apical dendrite, as described by Urbanczik and Senn, 2014. The cortical microcircuit with lateral â€˜inhibitionâ€™ that seeks to cancel the top-down feedback in order to extract the apical error is inspired by Sacramento et al., 2018 and Haider et al., 2021. The energy-based approach for describing error-backpropagation for weak nudging is borrowed from the Equilibrium Propagation algorithm (Scellier and Bengio, 2017) that we generalize from a steady-state algorithm to real-time computation in cross-cortical microcircuits. Our theory covers both cases of weak and strong output nudging. For strong nudging, it likewise generalizes the least-control principle (Meulemans et al., 2022) and the prospective configuration algorithm (Song et al., 2024) from a steady-state to a dynamic real-time version, linking to optimal feedback control (Todorov and Jordan, 2002). Finally, the apical activity of our pyramidal neurons can be seen in the tradition of predictive coding (Rao and Ballard, 1999), where cortical feedback connections try to explain away lower-level activities. Yet, different from classical predictive coding, our prediction errors are integrated with the soma, and these errors are prospective in time. The errors extrapolate from current to future activities, so that their integration improves the network output in real time. The combination of an energy-based model with prospective coding in which neuronal integration delays are compensated on the fly enters also in Haider et al., 2021.
The paper is organized as follows: we first define the prospective somatodendritic mismatch error, construct out of this the mismatch energy of a network, and â€˜minimizeâ€™ this energy to obtain the error-corrected, prospective voltage dynamics of the network neurons. We then show that the prospective error coding leads to an instantaneous and joint processing of low-pass filtered input signals and backpropagated errors. Applied to motor control, the instantaneous processing is interpreted as a moving equilibrium hypothesis according to which sensory inputs, network state, motor commands, and muscle feedback are in a self-consistent equilibrium at any point of the movement. We then derive a local learning rule that globally minimizes the somato-dendritic mismatch errors across the network, and show how this learning can be implemented through error-extracting cortical microcircuits and dendritic predictive plasticity.

RESULTS
Somato-dendritic mismatch errors and the Lagrangian of cortical circuits
We consider a network of neurons â€“ identified as pyramidal cells â€“ with firing rates ri(t) in continuous time t. The somatic voltage ui of pyramidal neuron i is driven by the close-by basal input current, âˆ‘jWijrj, with presynaptic rates rj and synaptic weights Wij, and an additional distal apical input ei that will be learned to represent a prospective prediction error at any moment in time (Figure 1a). While in classical rate-based neuron models the firing rate ri of a neuron is a function of the somatic voltage, Ï(ui), the NLA principle implies that the effective firing rate of a cortical neuron is prospective. More concretely, the formalism derives a firing rate that linearly extrapolates from Ï(ui) into the future with the temporal derivative, ri=Ï(ui)+Ï„ÏË™(ui), where ÏË™(ui) represents the temporal derivative of Ï(ui(t)). There is experimental evidence for such prospective coding in cortical pyramidal neurons where the instantaneous rate ri is in fact not only a function of the underlying voltage, but also a function of how quickly that voltage increases (see Figure 2a).
Figure 2: Prospective coding in cortical pyramidal neurons enables instantaneous voltage-to-voltage transfer. (a1) The instantaneous spike rate of cortical pyramidal neurons (top) in response to sinusoidally modulated noisy input current (bottom) is phase-advanced with respect to the input adapted from KÃ¶ndgen et al., 2008. (a2) Similiarly, in neuronal least-action (NLA), the instantaneous firing rate of a model neuron (r=Ï(u)+Ï„ÏË™(u), black) is phase-advanced with respect to the underlying voltage (u, red, postulating that the low-pass filtered rate is a function of the voltage, râ€¾=Ï(u)). (b) Dendritic input in the apical tree (here called eâ€¾) is instantaneously causing a somatic voltage modulation (u, modeling data from Ulrich, 2002). The low-pass filtering with Ï„ along the dendritic shaft is compensated by a lookahead mechanism in the dendrite (e=eâ€¾+Ï„eâ€¾Ë™). In (Ulrich, 2002) a phase advance is observed even with respect to the dendritic input current, not only the dendritic voltage, although only for slow modulations (as here). (c) While the voltage of the first neuron (u1) integrates the input rates rin from the past (bottom black upward arrows), the output rate r1 of that first neuron looks ahead in time, r1=Ï(u1)+Ï„ÏË™(u1) (red dashed arrows pointing into the future). The voltage of the second neuron (u2) integrates the prospective rates r1 (top black upwards arrows). By doing so, it inverts the lookahead operation, resulting in an instantaneous transfer from u1(t) to u2(t) (blue arrow and circles).
The second central notion of the theory is the prospective error ei, that we interpret as prospective somato-dendritic mismatch error in the individual network neurons, ei=(ui+Ï„uË™i)âˆ’âˆ‘jWijrj . It is defined as a mismatch between the prospective voltage, ui+Ï„uË™i, and the weighted prospective input rates, âˆ‘jWijrj. In the same way, as the firing rates rj linearly extrapolate into the future given the current-voltages uj of the presynaptic neurons j, the postsynaptic error is based on the linear extrapolation of its current voltage ui using its temporal derivative, ui+Ï„uË™i . If the prospective error ei is low-pass filtered with time constant Ï„, it takes the form eâ€¾i=uiâˆ’âˆ‘jWijrâ€¾j, where râ€¾j is the corresponding low-pass filtered firing rate of the presynaptic neuron j (that becomes a function of the presynaptic voltage, râ€¾j=Ï(uj) , see Methods, Sect. Euler-Lagrange equations as inverse low-pass filters). We refer to eâ€¾i as a somato-dendritic mismatch error of neuron that, as compared to ei, is non-prospective and instantaneous.
We next interpret the mismatch error eâ€¾i in terms of the morphology and biophysics of pyramidal neurons with basal and apical dendrites. While the error ei is formed in the apical dendrite, this error is low-pass filtered and added to the somatic voltage ui, that is also driven by the low-pass filtered basal input âˆ‘jWijrâ€¾j, so that  ui=âˆ‘jWijrâ€¾j+eâ€¾i. From the perspective of the basal dendrites, the low-pass filtered apical error eâ€¾i can be calculated as the difference between the somatic voltage and the own local low-pass filtered input, eâ€¾i=uiâˆ’âˆ‘jWijrâ€¾j. The somatic voltage ui is assumed to be sampled in the basal dendrite by the backpropagating acting potentials (Urbanczik and Senn, 2014; Spicher et al., 2017). The apical error now appears as a â€˜somato-basalâ€™ mismatch error, that both are summarized as a somato-dendritic mismatch error. It tells the difference between â€˜what a neuron does,â€™ which is based on the somatic voltage ui, and â€˜what the basal inputs think it should do,â€™ which is based on its own input âˆ‘Wijrâ€¾j (Figure 1a2). The two quantities may deviate because neuron i get additional â€˜unpredictedâ€™ apical inputs from higher-area neurons that integrate with the somatic voltage ui. What cannot be predicted in ui by the sensory-driven basal input remains as somato-basal (somato-dendritic) mismatch error eâ€¾i.
Associated with this mismatch error is the somatodendritic mismatch energy defined for each network neuron iâˆˆN as the squared mismatch error,
On a subset of output neurons of the whole network, OâŠ†N, a cost is defined as a function of the somatic voltage and some instructive reference signal such as targets or a reward. When a target trajectory uo*(t) is available, the cost is defined at each time point as a squared target error,
Much more general mismatch energies and cost functions are conceivable, for instance, errors of the form eâ€¾i=uiâˆ’fi(ğ’–,t) for general functions fi of the voltage vector ğ’– and of time, encompassing conductance-based neurons, but also further dynamic variables can be included such as threshold adaptation (see Appendix 6). The cost represents a performance measure for the entire network that produces the output voltages uo(t) in response to some input rates ğ’“in(t). The cost directly relates to behavioral or cognitive measures such as the ability of an animal or human to perform a particular task in real time. The target could be provided by explicit external supervision, for example, target movements in time encoded by uo*(t), it could represent an expected reward signal, or it could arise via self-supervision from other internal prediction errors.
We define the Lagrangian (or â€˜total energyâ€™) of the network as a sum across all mismatch energies and costs, weighted by the nudging strength Î² of the output neurons,
The low-pass filtered presynaptic rates, râ€¾j, also encompass the external input neurons. While in classical energy-based approaches, L is called the total energy, we call it the â€˜Lagrangianâ€™ because it will be integrated along real and virtual voltage trajectories as done in variational calculus (leading to the Euler-Lagrange equations, see below and Appendix 6). We â€˜prospectivelyâ€™ minimize L locally across a voltage trajectory, so that, as a consequence, the local synaptic plasticity for Wij will globally reduce the cost along the trajectory (Theorem 1 below).
Due to the prospective coding, the Lagrangian can be minimal at any moment in time while the network dynamics evolve. This is different from the classical predictive coding (Rao and Ballard, 1999) and energy-based approaches (Scellier and Bengio, 2017; Song et al., 2024), where a stimulus needs to be fixed in time while the network relaxes to a steady state, and only there the prediction error is minimized (see Appendix 3).

The least-action principle expressed for prospective firing rates
Motivated by the prospective firing in pyramidal neurons, we postulate that cortical networks strive to look into the future to prevent instantaneous errors. Each neuron tries to move along a trajectory that minimizes its own mismatch error eâ€¾i across time (Figure 1b). The â€˜neuronal currencyâ€™ with which each neuron â€˜tradesâ€™ with others to choose its own error-minimizing trajectory is the future discounted membrane potential,
The prospective voltages u~ are the â€˜canonical coordinatesâ€™ entering the NLA principle, and in these prospective coordinates the overall network searches for a â€˜least-action trajectoryâ€™. Since from u~ we can recover the instantaneous voltage via u=u~âˆ’Ï„u~Ë™ (see Appendix 2), we can replace u in the Lagrangian and obtain L as a function of our new prospective coordinates u~ and the â€˜velocitiesâ€™ u~Ë™, i.e.,L=L[ğ’–~,ğ’–~Ë™], where bold fonts represent vectors. Inspired by the least-action principle from physics, we define the neuronal action A as a time-integral of the Lagrangian,
The NLA principle postulates that the trajectory ğ’–~(t) keeps the action A stationary with respect to small variations Î´ğ’–~ (Figure 1b1). In other words, nature chooses a trajectory such that, when deviating a little bit from it, say by Î´ğ’–~, the value of A will not change (or at most up to second order in the variation), formally Î´A=0. The motivation to search for a trajectory that keeps the action stationary is borrowed from physics. The motivation to search for a stationary trajectory by varying the near-future voltages ğ’–~, instead of ğ’–, is assigned to the evolutionary pressure in biology to â€˜think ahead of time.â€™ To not react too late, internal delays involved in the integration of external feedback need to be considered and eventually need to be overcome. In fact, only for the â€˜prospective coordinatesâ€™ defined by looking ahead into the future, even when only virtually, will a real-time learning from feedback errors become possible (as expressed by our Theorems below).
The equations of motion that keep the action stationary with respect to these prospective coordinates are known to satisfy the Euler-Lagrange equations.
Applying these equations to our Lagrangian yields a prospective version of the classical leaky integrator voltage dynamics, with rates ğ’“ and errors ğ’† that are looking into the future (Methods, Sects. Euler-Lagrange equations as inverse low-pass filters, Deriving the network dynamics from the Euler-Lagrange equations),
The â€˜â‹…â€™ denotes the component-wise product, and the weight matrix splits into weights from input neurons and weights from network neurons, ğ‘¾=(ğ‘¾in,ğ‘¾net). While for output neurons a target error can be defined, eâ€¾o*=uo*âˆ’uo, for non-output neurons i no target exists and we hence set eâ€¾i*=0. In a control theoretic framework, the neuronal dynamics (Equation 7a) represent the state trajectory, and the adjoint error dynamics Equation 7b represent the integrated costate trajectory (Todorov, 2006).
From the point of view of theoretical physics, where the laws of motion derived from the least-action principle contain an acceleration term (as in Newtonâ€™s law of motion, like mxÂ¨=âˆ’x+F for a harmonic oscillator), one may wonder why no second-order time derivative appears in the NLA dynamics. As an intuitive example, consider driving into a bend. Looking ahead in time helps us to reduce the lateral acceleration by braking early enough, as opposed to braking only when the lateral acceleration is already present. This intuition is captured by minimizing the neuronal action A with respect to the discounted future voltages u~i instead of the instantaneous voltages ui. Keeping up an internal equilibrium in the presence of a changing environment requires looking ahead and compensating early for the predicted perturbations. Technically, the acceleration disappears because the Euler-Lagrange operator (Equation 6) turns into a lookahead-gradient operator, âˆ‚âˆ‚u~iâˆ’ddtâˆ‚âˆ‚u~Ë™i=(1+ddt)âˆ‚âˆ‚ui, since the u~Â¨i is absorbed via u~Ë™iâˆ’Ï„u~Â¨i=uË™i (see Methods, Sect. Euler-Lagrange equations as inverse low-pass filters, and Appendix 6 for the link to the least-action principle in physics).
Mathematically, the voltage dynamics in Equation 7a specifies an implicit differential equation since ğ’–Ë™(t) also appears on the right-hand side. This is because the prospective rates ğ’“=Ï(ğ’–)+Ï„ÏË™(ğ’–) include ğ’–Ë™ through ÏË™(u)=Ïâ€²(u)â‹…uË™. Likewise, the prospective errors ğ’†=ğ’†â€¾+Ï„ğ’†â€¾Ë™, with ğ’†â€¾ given in Equation 7b and plugged into Equation 7a, imply ğ’–Ë™ through eÂ¯Ë™(u)=eÂ¯â€²(u)â‹…uË™. Nevertheless, the voltage dynamics can be stably run by replacing ğ’–Ë™(t) on the right-hand side of Equation 7a with the temporal derivative ğ’–Ë™(tâˆ’dt) from the previous time step (technically, the Hessian (1âˆ’ğ‘¾ğ†â€²âˆ’ğ’†â€¾â€²) is required to be strictly positive definite, see Methods Sect. From implicit to explicit differential equations and Appendix 3). This ensures that the voltage dynamics of Equation 7a, Equation 7b can be implemented in cortical neurons with a prospective firing and a prospective dendritic error (see Figure 2).
The error expression in Equation 7b is reminiscent of error backpropagation Rumelhart et al., 1986 and can in fact be related (Methods, Sect. Deriving the error backpropagation formula). Formally, the errors are backpropagated via transposed network matrix, ğ‘¾netT, modulated by râ€¾iâ€², the derivative of râ€¾i=Ï(ui) with respect to the underlying voltage. While the transpose can be constructed with various local methods see Akrout et al., 2019; Max et al., 2022 in our simulations we mainly adhere to the phenomenon of feedback alignment (Lillicrap et al., 2016) and consider fixed and randomized feedback weights ğ‘© (unless stated differently). Recent control theoretical work is exploiting the same prospective coding technique as expressed in Equation 7a, Equation 7b to tackle general time-varying optimization problems see Simonetto et al., 2020 for a review and Appendix 3 for the detailed connection.

Prospective coding in neurons and instantaneous propagation
The prospective rates and errors entering via ğ’“ and ğ’† in the NLA (Equation 7a) are consistent with the prospective coding observed in cortical pyramidal neurons in vitro (KÃ¶ndgen et al., 2008). Upon sinusoidal current injection into the soma, the somatic firing rate is advanced with respect to its voltage (Figure 2a), effectively compensating for the delay caused by the current integration. Likewise, sinusoidal current injection in the apical tree causes a lag-less voltage response in the soma (Figure 2b, Ulrich, 2002). While the rates and errors in general can be reconstructed from their low-pass filterings via ğ’“=ğ’“â€¾+Ï„ğ’“â€¾Ë™ and ğ’†=ğ’†â€¾+Ï„ğ’†â€¾Ë™, they become prospective in time because ğ’“â€¾ and ğ’†â€¾ are themselves instantaneous functions of the voltage ğ’–, and hence ğ’“ and ğ’† depend on ğ’–Ë™. The derivative of the membrane potential implicitly also appears in the firing mechanism of Hodgkin-Huxley-type conductances, with a quick depolarization leading to a stronger sodium influx due to the dynamics of the gating variables (Hodgkin and Huxley, 1952). This advances the action potential as compared to a firing that would only depend on ğ’–, not ğ’–Ë™, giving an intuition of how such a prospective coding may arise. A similar prospective coding has been observed for retinal ganglion cells (Palmer et al., 2015) and cerebellar Purkinje cells (Ostojic et al., 2015), making a link from the visual input to the motor control.
To understand the instantaneous propagation through the network, we low-pass filter the dynamic equation ğ’–+Ï„ğ’–Ë™=ğ‘¾ğ’“+ğ’† (obtained by rearranging Equation 7a), with ğ’†â€¾ given by Equation 7b, to obtain the somatic voltage ğ’–=ğ‘¾ğ’“â€¾(ğ’–)+ğ’†â€¾(ğ’–). At any point in time, the voltage is in a moving equilibrium between forward and backpropagating inputs. Independently of the network architecture, whether recurrent or not, the output is an instantaneous function of the low-pass filtered input and a putative correction towards the target, ğ’–ğ’(t)=ğ‘­W(ğ’“â€¾in(t),ğ’†â€¾ğ’*(t)), see Figure 2C and Methods, Sect. Proving theorem 1 (rt-DeEP). The mapping again expresses an instantaneous propagation of voltages throughout the network in response to both, the low-pass filtered input ğ’“â€¾in and feedback error ğ’†â€¾ğ’*. This instantaneity is independent of the network size, and in a feed-forward network is independent of its depths (see also Haider et al., 2021, where the instantaneity is on the rates, not the voltages). In the absence of the look-ahead activity, each additional layer would slow down the network relaxation time.
Notice that an algorithmic implementation of the time-continuous dynamics of a N-layer feedforward network would still need N calculation steps until information from layer 1 reaches layer N. However, this does not imply that an analog implementation of the prospective dynamics will encounter delays. To see why, consider a finite step-change Î”ğ’–1 in the voltage of layer 1. In the absence of the look-ahead, Î”ğ’–1 was mapped within the infinitesimal time interval dt to an infinitesimal change dğ’–2 in the voltages of layer 2. But with a prospective firing rate, r1=Ï(u1)+Ï„Ïâ€²(u1)â‹…uË™1, a step-change Î”ğ’–1 translates to a delta-function in ğ’“1, this in turn to a step-change in the low-pass filtered rates Î”ğ’“â€¾1, and therefore within dt to a step-change Î”ğ’–2 in the voltages ğ’–2 of the postsynaptic neurons (Figure 2c). Iterating this argument, a step-change Î”ğ’–1 propagates â€˜instantaneouslyâ€™ through N layers within the â€˜infinitesimalâ€™ time interval Ndt to a step-change Î”ğ’–N in the last layer. When run in a biophysical device in continuous time that exactly implements the dynamical Equation 7a, the implementation becomes an instantaneous computation (since dtâ†’0). Yet, in a biophysical device, information has to be moved across space. This typically introduces further propagation delays that may not be captured in our formalism where low-pass filtering and prospective coding cancel each other exactly. Nevertheless, analog computation in continuous time, as formalized here, offers an idea to â€˜instantaneouslyâ€™ realize an otherwise time-consuming numerical recipe run on time-discrete computing systems that operate with a finite clock cycle.

Prospective control and the moving equilibrium hypothesis
Crucially, at the level of the voltage dynamics (Equation 7a) the correction is based on the prospective error ğ’†. This links our framework to optimal control theory and motor control where delays are also taken into account, so that a movement can be corrected early enough (Wolpert and Ghahramani, 2000; Todorov and Jordan, 2002; Todorov, 2004). The link between energy-based models and optimal control was recently drawn for strong nudging (Î²â†’âˆ) to learn individual equilibrium states (Meulemans et al., 2022). Our prospective error ğ’†(t) appears as a â€˜controllerâ€™ that, when looking at the output neurons, pushes the voltage trajectories toward the target trajectories. Depending on the nudging strength Î², the control is tighter or weaker. For infinitely large Î², the voltages of the output neurons are clamped to the time-dependent target voltages, uo=uo* (implying eo*=0), while their errors, eâ€¾o=uoâˆ’(ğ‘¾ğ’“â€¾)o, instantaneously correct all network neurons. For small Î², the output voltages are only weakly controlled, and they are dominated by the forward input, uoâ‰ˆ(Wğ’“â€¾)o.
To show how the NLA principle with the prospective coding globally maps to cortico-spinal circuits we consider the example of motor control. In the context of motor control, our network mapping ğ’–ğ’=ğ‘­W(ğ’“â€¾in,ğ’†â€¾ğ’*) can be seen as a forward internal model that quickly calculates an estimate of the future muscle length ğ’–ğ’ based on some motor plans, sensory inputs, and the current proprioceptive feedback (Figure 3a). Forward models help to overcome delays in the execution of the motor plan by predicting the outcome, so that the intended motor plans and commands can be corrected on the fly (Kawato, 1999; Wolpert and Ghahramani, 2000).
Figure 3: Moving equilibrium hypothesis for motor control and real-time learning of cortical activity. (a) A voluntary movement trajectory can be specified by the target length of the muscles in time, ğ’–ğ’*, encoded through the Î³-innervation of muscle spindles, and the deviation of the effective muscle lengths from the target, ğ’–ğ’âˆ’ğ’–ğ’*=âˆ’ğ’†â€¾ğ’*. The Ia-afferents emerging from the spindles prospectively encode the error, so that their low-pass filtering is roughly proportional to the length deviation, truncated at zero (red). The moving equilibrium hypothesis states that the low-pass filtered input ğ’“â€¾in, composed of the movement plan ğ’“â€¾inplan and the sensory input (here encoding the state of the plant e.g., through visual and proprioceptive input, ğ’“â€¾invis and ğ’“â€¾inprop), together with the low-pass filtered error feedback from the spindles, ğ’†â€¾ğ’*, instantaneously generate the muscle lengths, ğ’–ğ’=ğ‘­W(ğ’“â€¾in,ğ’†â€¾ğ’*), and are thus at any point in time in an instantaneous equilibrium (defined by Equation 7a, Equation 7b). (b1) Intracortical intracortical electroencephalogram (iEEG) activity recorded from 56 deep electrodes and projected to the brain surface. Red nodes symbolize the 56 iEEG recording sites modeled alternately as input or output neurons, and blue nodes symbolize the 40 â€˜hiddenâ€™ neurons for which no data is available, but used to reproduce the iEEG activity. (b2) Corresponding NLA network. During training, the voltages of the output neurons were nudged by the iEEG targets (black input arrows, but for all red output neurons). During testing, nudging was removed for 14 out of these 56 neurons (here, represented by neurons 1, 2, 3). (c1) Voltage traces for the 3 example neurons in a2, before (blue) and after (red) training, overlaid with their iEEG target traces (gray). (c2) Total cost, integrated over a window of 8 s of the 56 output nodes during training with sequences of the same duration. The cost for the test sequences was evaluated on a 8 s window not used during training.
The observation that muscle spindles prospectively encode the muscle length and velocity (Dimitriou and Edin, 2010) suggests that the prospective coding in the internal forward model mirrors the prospective coding in the effective forward pathway. This forward pathway leads from the motor plan to the spindle feedback, integrating also cerebellar and brainstem feedback (Kawato, 1999). Based on the motor plans, the intended spindle lengths and the effective muscle innervation are communicated via a descending pathway to activate the Î³- and Î±-motoneurons, respectively (Li et al., 2015). The mapping from the intended arm trajectory to the intended spindle lengths via Î³-innervation is mainly determined by the joint geometry. The mapping from the intended arm trajectory to the force-generating Î±-innervation, however, needs to also take account of the internal and external forces, and this is engaging our network ğ‘¾.
When we prepare an arm movement, spindles in antagonistic muscle pairs that measure the muscle length are tightened or relaxed before the movement starts (Papaioannou and Dimitriou, 2021). According to the classical equilibrium-point hypothesis (Feldman and Levin, 2009; Latash, 2010), top-down input adjusts the activation threshold of the spindles through (Î³-) innervation from the spinal cord so that slight deviations from the equilibrium position can be signaled (Figure 3a). We postulate that this Î³-innervation acts also during the movement, setting an instantaneous target ğ’–ğ’*(t) for the spindle lengths. The effective lengths of the muscle spindles is ğ’–ğ’, and the spindles are prospectively signaling back the deviation from the target through the Ia-afferents (Dimitriou and Edin, 2010; Dimitriou, 2022). The low-pass filtered Ia-afferents may be approximated by a threshold-nonlinearity, Ia=Î²âŒŠğ’–ğ’âˆ’ğ’–ğ’*âŒ‹+, with Î² being interpreted as spindle gain (Latash, 2018). Combining the feedback from agonistic and antagonistic muscle pairs allows for extracting the scaled target error Î²ğ’†â€¾ğ’*=Î²(ğ’–ğ’*âˆ’ğ’–ğ’). Taking account of the prospective feedback, we postulate the moving equilibrium hypothesis according to which the instructional inputs, ğ’“â€¾in, the spindle feedback, Î²ğ’†â€¾ğ’*, and the muscle lengths, ğ’–ğ’, are at any point of the movement in a dynamic equilibrium. The moving equilibrium hypothesis extends the classical equilibrium-point hypothesis from the spatial to the temporal domain (for a formal definition of a moving equilibrium see Methods, Sect. From implicit to explicit differential equations).
Prediction errors are also reduced when motor units within a muscle are recruited according to the size principle (Senn et al., 1997), which itself was interpreted in terms of the physical least-action principle (Senn et al., 1995). With regard to the interpretation of the prospective feedback error ğ’†ğ’* as spindle activity, it is worth noticing that in humans the spindle activity is not only ahead of the muscle activation (Dimitriou and Edin, 2010), but also share the property of a motor error (Dimitriou, 2016). The experiments show that during the learning of a gated hand movement, spindle activity is initially stronger when making movement errors, and it returns back to baseline with the success of learning. This observation is consistent with the NLA principle, saying that the proprioceptive prediction errors are minimized through the movement learning. We next address how the synaptic strengths ğ‘¾ involved in producing the muscle length can be optimally adapted to capture this learning.

Local plasticity at basal synapses minimizes the global cost in real time
The general learning paradigm starts with input time series rin(t),i and target time series uo*(t), while assuming that the target series are an instantaneous function of the low-pass filtered input series, ğ’–ğ’*(t)=ğ‘­*(ğ’“in(t)). The low-pass filtering in the individual inputs could be with respect to any time constant Ï„in,i* (that may also be learned, see Appendix 2). Yet, for simplicity, we assume the same time constant Ï„ for low-pass filtering the rates of the network neurons and input neurons. The goal of learning is to adapt the synaptic strengths ğ‘¾ in the student network so that this moves towards the target mapping, ğ‘­Wâ†’ğ‘­*. The local synaptic plasticity will also reduce the global cost C defined on the output neurons o in terms of the deviation of the voltage from the target, uo*âˆ’uo (Equation 2).
The problem of changing synaptic weights to correct the behavior of downstream neurons, potentially multiple synapses away, is typically referred to as the credit assignment problem and is notoriously challenging in physical or biological substrates operating in continuous time. A core aspect of the NLA principle is how it relates the global cost C to the Lagrangian L and eventually to somato-dendritic prediction errors ğ’†â€¾ that can be reduced through local synaptic plasticity ğ‘¾Ë™. We define this synaptic plasticity as a partial derivative of the Lagrangian with respect to the weights, ğ‘¾Ë™âˆâˆ’âˆ‚Lâˆ‚ğ‘¾=ğ’†â€¾ğ’“â€¾T. Since the somatodendritic mismatch error is ğ’†â€¾=ğ’–âˆ’ğ‘¾ğ’“â€¾, this leads to the local learning rule of the form â€˜postsynaptic error times low-pass filtered presynaptic rateâ€™,
The plasticity rule runs simultaneously to the neuronal dynamics in the presence of a given nudging strength Î² that tells how strongly the voltage of an output neuron is pushed towards the target, uoâ†’uo*. The learning rule is local in space since ğ‘¾ğ’“â€¾ is represented as a voltage of the basal dendrites, and the somatic voltage ğ’– may be read out at the synaptic site on the basal dendrite from the backpropagating action potentials that sample ğ’– at a given time (Urbanczik and Senn, 2014). The basal voltage ğ‘¾ğ’“â€¾ becomes the dendritic prediction of the somatic activity ğ’–, interpreting Equation 8 as â€˜dendritic predictive plasticityâ€™.
We have derived the neuronal dynamics as a path that keeps the action stationary. Without an external teaching signal, the errors vanish, and the voltage trajectory wriggles on the bottom of the energy landscape (L=0, Figure 1b2). If the external nudging is turned on, Î²&gt;0, errors emerge and hills grow out of the landscape. The trajectory still tries to locally minimize the action, but it is lifted upwards on the hills (L&gt;0, Figure 1b2). Synaptic plasticity reshapes the landscape so that, while keeping Î² fixed, the errors are reduced and the landscape again flattens. The transformed trajectory settles anew in another place (inside the â€˜volcanoâ€™ in 1b2). Formally, the local plasticity rule (Equation 8) is shown to perform gradient descent on the Lagrangian and hence on the action. In the energy landscape picture, plasticity â€˜shovels offâ€™ energy along the voltage path so that this is lowered most efficiently. The error that is back-propagated through the network tells at any point on the voltage trajectory how much to â€˜digâ€™ in each direction, i.e., how to adapt the basal input in each neuron in order to optimally lower the local error.
The following theorem tells that synaptic plasticity ğ‘¾Ë™ pushes the network mapping ğ’–ğ’=ğ‘­W(ğ’“â€¾in) towards the target mapping ğ’–ğ’*=ğ‘­*(ğ’“â€¾in) at any moment in time. The convergence of the mapping is a consequence of the fact the plasticity reduces the Lagrangian L=EM+Î²C along its gradient.

Theorem 1 (real-time dendritic error propagation, rt-DeEP)
Consider an arbitrary network ğ‘¾ with voltage and error dynamics following Equation 7a, Equation 7b. Then the local plasticity rule ğ‘¾Ë™âˆğ’†â€¾ğ’“â€¾T Equation 8, acting at each moment along the voltage trajectories, is gradient descent
The gradient statements hold at any point in time (long enough after initialization), even if the input trajectories ğ’“in(t) contain delta functions and the target trajectories ğ’–ğ’*(t) contain step functions.
Loosely speaking, the NLA enables the network to localize in space and time an otherwise global problem: what is good for a single neuron (the local plasticity) becomes good for the entire network (the gradient on the global cost). Learning is possible at any point in time along the trajectory because the NLA inferred a prospective voltage dynamics expressed in prospective firing rates ri and prospective errors ei of the network neurons. In the limit of strong nudging (Î²â†’âˆ), the learning rule performs gradient descent on the mismatch energies EMi in the individual neurons. If the network architecture is powerful enough so that after learning all the mismatch energies vanish, EMi=0, then the cost will also vanish, C=12â€–ğ’–ğ’*âˆ’ğ’–ğ’â€–2=0. This is because for the output neurons, the mismatch error includes the target error (Equation 7b). In the limit of weak nudging (Î²â†’0), the learning rule performs gradient descent on C, and with this also finds a local minimum of the mismatch energies.
In the case of weak nudging and a single steady-state equilibrium, the NLA algorithm reduces to the Equilibrium Propagation algorithm (Scellier and Bengio, 2017) that minimizes the cost C for a constant input and a constant target. In the case of strong nudging and a single steady-state equilibrium, the NLA principle reduces to the Least-Control Principle (Meulemans et al., 2022) that minimizes the mismatch energy EM for a constant input and a constant target, with the apical prediction error becoming the prediction error from standard predictive coding (Rao and Ballard, 1999). While in the Least-Control Principle, the inputs and outputs are clamped to fixed values, the output errors are backpropagated and the network equilibrates in a steady state where the corrected network activities reproduce the clamped output activities. This state is called the â€˜prospective configurationâ€™ in Song et al., 2024 because neurons deep in the network are informed about the distal target already during the inference, and are correspondingly adapted to be consistent with this distal target. In the NLA principle, after an initial transient, the network always remains in the moving equilibrium due to the prospective coding. While inputs and targets dynamically change, the network moves along a continuous sequence of prospective configurations.
In the motor control example, the theorem tells that a given target motor trajectory ğ’–ğ’*(t) is learned to be reproduced with the forward model ğ’–ğ’(t)=ğ‘­W(ğ’“â€¾in(t)), by applying the dendritic predictive plasticity for the network neurons (Equation 8). We next exemplify the theory by looking into the brain, reproducing cortical activity, and showing how a multi-layer cortical network can learn a sensory-motor mapping while staying in a moving equilibrium throughout the training.

Reproducing intracortical EEG recordings and recognizing handwritten digits
As an illustration, we consider a recurrently connected network that learns to represent intracortical electroencephalogram (iEEG) data from epileptic patients (Figure 3b). For each electrode, we assign a neuron within this network to represent the activity of the cell cluster recorded in the corresponding iEEG signal via its membrane potential. During learning, a randomly selected subset of electrode neurons are nudged towards the target activity from recorded data while learning to be reproduced by the other neurons. After learning, we can present only a subset of electrode neurons with previously unseen recordings and observe how the activity of the other neurons closely matches the recordings of their respective electrodes (Figure 3c). The network derived from NLA is thus able to learn complex correlations between signals evolving in real-time by embedding them in a recurrent connectivity structure.
As an example of sensory-motor processing in the NLA framework, we next consider a well-studied image recognition task, here reformulated in a challenging time-continuous setting, and interpreted as a motor task where 1 out of 10 fingers has to be bent upon seeing a corresponding visual stimulus (see Figure 3). In the context of our moving equilibrium hypothesis, we postulate that during the learning phase, but not the testing phase, an auditory signal identifies the correct finger and sets the target spindle lengths of 10 finger flexors, ğ’–ğ’*(t). The target spindle length encodes the desired contraction of a flexor muscle in the correct finger upon the visual input ğ’“in(t), and a corresponding relaxation for the nine incorrect fingers.
We train a hierarchical three-layer network on images of handwritten digits (MNIST, LeCun, 1998), with image presentation times between 0.5Ï„ (=5 ms) and 20Ï„ (=200 ms, with Ï„=10 the membrane time constant). Figure 4a-c depict the most challenging scenario with the shortest presentation time. Synaptic plasticity is continuously active, despite the network never reaching a temporal steady state (Figure 4b1). Due to the lookahead firing rates in the NLA, the mismatch errors eâ€¾i(t) represent the correct gradient and propagate without lag throughout the network. As a consequence, our mismatch errors are almost equal to the errors obtained from classical error backpropagation applied at each time step to the purely forward network (i.e. the network that suppresses the error-correction ğ’†â€¾ of the voltage and instead considers the â€˜classicalâ€™ voltage ğ’–l=ğ‘¾lÏ(ğ’–lâˆ’1) only, see blue dots in Figure 4b2). The network eventually learned to implement the mapping ğ’–ğ’=ğ‘­W(ğ’“â€¾in)â‰ˆğ’–ğ’* with a performance comparable to error-backpropagation at each dt, despite the short presentation time of only 5 ms (Figure 4c1). The approximation is due to the fact that the NLA learns an instantaneous mapping from the low-pass filtered input rates ğ’“â€¾in to the output voltage ğ’–ğ’, while the mapping from the original input rates ğ’“in to the voltages ğ’–1 of the first-layer neurons (and hence also to the output voltages ğ’–ğ’) is delayed by Ï„in. Since in the simulations, the target voltages ğ’–ğ’* were switched instantaneously with ğ’“in (and not with ğ’“â€¾in), however, a mismatch error between ğ’–ğ’ and ğ’–ğ’* remains for stimulus presentation times shorter than Ï„in (Figure 4c2). The Latent Equilibrium (Haider et al., 2021) avoids these temporal limitations by implementing an instantaneous mapping on the rates instead on the voltages (Methods, Sect. From implicit to explicit differential equations).
Figure 4: On-the-fly learning of finger responses to visual input with real-time dendritic error propagation (rt-DeEP). (a) Functionally feedforward network with handwritten digits as visual input (ğ’“in(2)(t) in Figure 3a, here from the MNIST data set, 5 ms presentation time per image), backprojections enabling credit assignment, and activity of the 10 output neurons interpreted as commands for the 10 fingers (forward architecture: 784Ã—500Ã—10 neurons). (b) Example voltage trace (b1) and local error (b2) of a hidden neuron in neuronal least-action (NLA) (red) compared to an equivalent network without lookahead rates (orange). Note that neither network achieves a steady state due to the extremely short input presentation times. Errors are calculated via exact backpropagation, i.e., by using the error backpropagation algorithm on a pure feedforward NLA network at every simulation time step (with output errors scaled by Î²), shown for comparison (blue dots). (c) Comparison of network models during and after learning. Color scheme as in (b). (c1) The test error under NLA evolves during learning on par with classical error backpropagation performed each Euler dt based on the feedforward activities. In contrast, networks without lookahead rates are incapable of learning such rapidly changing stimuli. (c2) With increasing presentation time, the performance under NLA further improves, while networks without lookahead rates stagnate at high error rates. This is caused by transient, but long-lasting misrepresentation of errors following stimulus switches: when plasticity is turned off during transients and is only active in the steady state, comparably good performance can be achieved (dashed orange). (d) Receptive fields of 6 hidden-layer neurons after training, demonstrating that even for very brief image presentation times (5ms), the combined neuronal and synaptic dynamics are capable of learning useful feature extractors such as edge filters.
The instantaneous voltage propagation relieves an essential constraint of previous models of bio-plausible error backpropagation (e.g. Scellier and Bengio, 2017; Whittington and Bogacz, 2017; Sacramento et al., 2018), with reviews (Richards et al., 2019; Whittington and Bogacz, 2019; Lillicrap et al., 2020): without lookahead firing rates, networks need much longer to correctly propagate errors across layers, with each layer roughly adding another membrane time constant of 10 ms, and thus cannot cope with realistic input presentation times. In fact, in networks without lookahead output, learning is only successful if plasticity is switched off while the network dynamics did not reach a stationary state during a stimulus presentation interval (Figure 4c2). Notice also that the prospective coding is necessary to keep the network activity stable for an instantaneous processing of the sensory input. If, in the absence of prospective coding, we would only shrink the membrane time constant to 0, the recurrent error processing would become unstable (see Appendix 3).

Implementation in cortical microcircuits
So far, we did not specify how errors ğ’† appearing in the differential equation for the voltage (Equation 7a) are transmitted across the network in a biologically plausible manner. Building on Sacramento et al., 2018, we propose a cortical microcircuit to enable this error transport, with all neuron dynamics evolving according to the NLA principle. Although the idea applies to arbitrarily connected networks, we use the simpler case of functionally feedforward networks to illustrate the flow of information in these microcircuits (Figure 5a).
Figure 5: Hierarchical plastic microcircuits implement real-time dendritic error learning (rt-DeEL). (a) Microcircuit with â€˜top-downâ€™ input (originating from peripheral motor activity, blue line) that is explained away by the lateral input via interneurons (dark red), with the remaining activity representing the error eâ€¾l. Plastic connections are denoted with a small red arrow and nudging with a dashed line. (b1) Simulated network with 784-300-10 pyramidal-neurons and a population of 40 interneurons in the hidden layer used for the MNIST learning task where the handwritten digits have to be associated with the 10 fingers. (b2) Test errors for rt-DeEL with joint tabula rasa learning of the forward and lateral weights of the microcircuit. A similar performance is reached as with classical error backpropagation. For comparability, we also show the performance of a shallow network (dashed line). (b3) Angle derived from the Frobenius norm between the lateral pathway ğ‘¾IPlğ‘¾PIl and the feedback pathway ğ‘©lğ‘¾l+1. During training, both pathways align to allow correct credit assignment throughout the network. Indices are dropped in the axis label for readability.
For such an architecture, pyramidal neurons in area l (that is a â€˜layerâ€™ of the feedforward network) are accompanied by a pool of interneurons in the same layer (area). The dendrites of the interneurons integrate in time (with time constant Ï„) lateral input from pyramidal neurons of the same layer (ğ’“l) through plastic weights ğ‘¾IPl. Additionally, interneurons receive â€˜top-down nudgingâ€™ from pyramidal neurons in the next layer through randomly initialized and fixed back projecting synapses ğ‘©IPl targeting the somatic region, and interneuron nudging strength Î²I. The notion of â€˜top-downâ€™ originates from the functionally feed-forward architecture leading from sensory to â€˜higher cortical areas.â€™ In the context of motor control, the highest â€˜areaâ€™ is the last stage controlling the muscle lengths, being at the same time the first stage for the proprioceptive input (Figure 3a).
According to the biophysics of the interneuron, the somatic membrane potential becomes a convex combination of the two types of afferent input (Urbanczik and Senn, 2014),
In the biological implementation, the feedback input is mediated by the low-pass filtered firing rates ğ’“â€¾l+1=Ï(ğ’–l+1), not by ğ’–l+1 as expressed in the above equation. Yet, we argue that for a threshold-linear Ï the â€˜top-down nudgingâ€™ by the rate ğ’“â€¾l+1 is effectively reduced to a nudging by the voltage ğ’–l+1. This is because errors are only backpropagated when the slope of the transfer function is positive, ğ’“l+1â€²&gt;0, and hence when the upper-layer voltage is in the linear regime. For more general transfer functions, we argue that short-term synaptic depression may invert the low-pass filtered presynaptic rate back to the presynaptic membrane potential, ğ’“â€¾l+1â†’ğ’–l+1, provided that the recovery time constant Ï„ matches the membrane time constant (see end of Results and Appendix 1).
Apical dendrites of pyramidal neurons in each layer receive top-down input from the pyramidal population in the upper layer through synaptic weights ğ‘©l. These top-down weights could be learned to predict the lower-layer activity (Rao and Ballard, 1999) or to become the transposed of the forward weight matrix (ğ‘©l=ğ‘¾l+1T, Max et al., 2022), but for simplicity, we randomly initialized them and keep them fixed (Lillicrap et al., 2020). Besides the top-down projections, the apical dendrites also receive lateral input via an interneuron population in the same layer through synaptic weights âˆ’ğ‘¾PIl that are plastic and will be learned to obtain suitable dendritic errors. The â€˜-â€™ sign is suggestive of these interneurons to subtract away the top-down input entering through ğ‘©l (while the weights can still be positive or negative). Assuming again a conversion of rates to voltages, also for the inhibitory neurons that may operate in a linear regime, the overall apical voltage becomes
What cannot be explained away from the top-down input ğ‘©lğ’–l+1 by the lateral feedback, âˆ’ğ‘¾PIlğ’–Il, remains as dendritic prediction error ğ’†â€¾lA in the apical tree (Figure 5a). If the top-down and lateral feedback weights are learned as outlined next, these apical prediction errors take the role of the backpropagated errors in the classical backprop algorithm.
To adjust the interneuron circuit in each layer (â€˜areaâ€™), the synaptic strengths from pyramidal-to-interneurons, ğ‘¾IPl, are learned to minimize the interneuron mismatch energy, ElIP=12â€–ulIâˆ’WlIPrÂ¯lâ€–2. The interneurons, while being driven by the lateral inputs ğ‘¾IPlğ’“â€¾l, learn to reproduce the upper-layer activity that also nudges the interneuron voltage. Learning is accomplished if the upper-layer activity, in the absence of an additional error on the upper layer, is fully reproduced in the interneurons by the lateral input.
Once the interneurons learn to represent the â€˜error-freeâ€™ upper-layer activity, they can be used to explain away the top-down activities that also project to the apical trees. The synaptic strengths from the inter-to-pyramidal neurons, ğ‘¾PIl, are learned to minimize the apical mismatch energy, ElPI=12â€–eÂ¯lAâ€–2=12â€–Blul+1âˆ’WlPIulIâ€–2. While in the absence of an upper-layer error, the top-down activity ğ‘©lğ’–l+1 can be fully cancelled by the interneuron activity ğ‘¾PIlğ’–Il, a neuron-specific error will remain in the apical dendrites of the lower-level pyramidal neurons if there is an error endowed in the upper-layer neurons. Gradient descent learning on these two energies results in the learning rules for the P-to-I and I-to-P synapses,
The following theorem on dendritic error learning tells that the plasticity in the lateral feedback loop leads to an appropriate error representation in the apical dendrites of pyramidal neurons.

Theorem 2 (real-time dendritic error learning, rt-DeEL)
Consider a cortical microcircuit composed of pyramidal and interneurons, as illustrated in Figure 5a, with more interneurons in layer (â€˜cortical areaâ€™) l than pyramidal neurons in layer l+1, and with adaptable pyramidal-to-inhibitory weights ğ‘¾IPl within the same layer that are nudged through top-down weights ğ‘©IPl, see Methods, Sect. Proving theorem 2 (rt-DeEL). Then, for suitable top-down nudging, learning rates, and initial conditions, the inhibitory-to-pyramidal synapses ğ‘¾PIl within each layer l (Equation 11) evolve such that the lateral feedback circuit aligns with the bottom-up-top-down feedback circuit,
After this horizontal-to-vertical circuit alignment, the apical voltages ğ’†â€¾lA=ğ‘©lğ’–l+1âˆ’ğ‘¾PIlğ’–Il of the layer-l pyramidal neurons (Equation 13) represent the â€˜B-backpropagatedâ€™ errors, ğ’†â€¾lA=ğ‘©lğ’†â€¾l+1. When modulated by the postsynaptic rate derivatives, ğ’“â€¾lâ€²=ğ†â€²(ğ’–â€¾l), the apical voltages yield the appropriate error signals
for learning the forward weights ğ‘¾l according to ğ‘¾Ë™lâˆğ’†â€¾lğ’“â€¾lâˆ’1T Equation 8.
The back projecting weights can also be learned by a local real-time learning rule to become transpose of the forward weights, ğ‘©l=ğ‘¾l+1T (Max et al., 2022). In this case, the error signals ğ’†â€¾l learned in the apical dendrites according to the above Theorem (Equation 13) represent the gradient errors ğ’†â€¾ appearing in the real-time dendritic error propagation (rt-DeEP, Theorem 1). There, the errors ğ’†â€¾ drive the gradient plasticity of the general weight matrix ğ‘¾, split up here into the forward weights ğ‘¾l to a layer l (for l=1,..,N).

Simultaneously learning apical errors and basal signals
Microcircuits following these neuronal and synaptic dynamics are able to learn the classification of hand-written digits from the MNIST dataset while learning the apical signal representation (Figure 5b1, b2). In this case, feedforward weights ğ‘¾l and lateral weights WlPI and WlIP are all adapted simultaneously. Including the WlIPË™-plasticity (by turning on the interneuron nudging from the upper layer, Î²I&gt;0 in Equation 9), greatly speeds up the learning.
With and without WlIPË™-plasticity, the lateral feedback via interneurons (with effective weight WlIPWlPI) learns to align with the forward-backward feedback via upper layer pyramidal neurons (with effective weight ğ‘©lğ‘¾l+1, Figure 5b3). The microcircuit extracts the gradient-based errors (Equation 13), while the forward weights use these errors to reduce these errors to first minimize the neuron-specific mismatch errors, and eventually the output cost.
Since the apical voltage ğ’†â€¾lA appears as a postsynaptic factor in the plasticity rule for the interneurons (WlPIË™), this I-to-P plasticity can be interpreted as Hebbian plasticity of inhbitory neurons, consistent with earlier suggestions (Vogels et al., 2011; Bannon et al., 2020). The plasticity WlIPË™ of the P-to-I synapses, in the same way as the plasticity for the forward synapses ğ‘¾Ë™l, can be interpreted as learning from the dendritic prediction of somatic activity (Urbanczik and Senn, 2014).
Crucially, by choosing a large enough interneuron population, the simultaneous learning of the lateral microcircuit and the forward network can be accomplished without fine-tuning of parameters. As an instance in case, all weights shared the same learning rate. Such stability bolsters the biophysical plausibility of our NLA framework and improves over the previous, more heuristic approach (Sacramento et al., 2018; Mesnard et al., 2019). The stability may be related to the nested gradient descent learning according to which somatic and apical mismatch errors in pyramidal neurons, and somatic mismatch errors in inhibitory neurons are minimized.
Finally, since errors are defined at the level of membrane voltages (Equation 11), synapses need a mechanism by which they can recover the presynaptic voltage errors from their afferent firing rates. While for threshold-linear transfer functions the backpropagated voltage errors translate into rate errors (Appendix 1), more general neuronal nonlinearities must be matched by corresponding synaptic nonlinearities. Pfister et al., 2010 have illustrated how spiking neurons can leverage short-term synaptic depression to estimate the membrane potential of their presynaptic partners. Here, we assume a similar mechanism in the context of our rate-based neurons. The monotonically increasing neuronal activation function, ğ’“â€¾l+1=Ï(ğ’–l+1), can be approximately compensated by a vesicle release probability that monotonically decreases with the low-pass filtered presynaptic rate ğ’“â€¾l+1 (see Appendix 1 and Appendix 1â€”figure 1). If properly matched, this leads to a linear relationship between the presynaptic membrane potential ğ’–l+1 and the postsynaptic voltage contribution.


DISCUSSION
We introduced a least-action principle to neuroscience for deriving the basic laws of the voltage and synaptic dynamics in networks of cortical neurons. The approach is inspired by the corresponding principle in physics where basic laws of motion are derived across the various scales. While in physics the action is defined as the time-integral of the kinetic minus potential energy, we define the action as the time-integral of instantaneous somatodendritic mismatch errors across network neurons plus a behavioral error. The â€˜kineticsâ€™ of a voltage trajectory only arises because we postulate that the action along a trajectory is minimized with respect to future voltages, not the instantaneous voltage, as would be done in physics. The postulate implies a prospective voltage dynamics that look ahead in time, together with prospective local errors, in order to minimize the action and hence the somatodendritic mismatch errors. The prospective errors nudge the firing of pyramidal neurons deep in the brain, so that motor neurons improve the output of the network right in time. A putative behavioral error, encoded in the motor feedback, propagates back through the network and produces prospective corrections of the pyramidal neuron activities that effectively manifest in instantaneous corrections of the motor trajectory. Through this prospective coding, the sensory stream, the deep network activity, and the motor feedback are in sync at any moment in time. We formulated the dynamic synchronization as a â€˜moving equilibrium hypothesisâ€™, referring to the classical equilibrium point hypothesis for motor control (Feldman and Levin, 2009; Latash, 2010). More generally, the brain activity formed by the prospective firing of cortical pyramidal neurons is in a moving equilibrium while converting sensory input streams into motor outputs, consistent with prospective sensory processing in the human cortex (Blom et al., 2020).
Because the neuronal dynamics derived from the global NLA principle is in a moving equilibrium, the prospective dendritic errors that globally correct the output trajectory are also suited to instruct local synapatic plasticity in the dendrites. In fact, working down the apical errors by adapting the sensory-driven synapses on the basal dendrites reduces the global output errors in real time. The apical errors are extracted from the top-down feedback via lateral â€˜inhibitionâ€™ that tries to cancel the top-down signal. This top-down feedback includes activity from a putative erroneous motor output that was not foreseen by the local inhibition and thus survives as a local apical error. Given the prospective coding of the pyramidal neurons, the dendritic errors are also prospective and thus able to induce the correct error-minimizing plasticity online, while stimuli and targets continuously change.
The NLA principle as a bottom-up theory from neurons to behavior
To show that the NLA principle offers a viable program for a formalization of neuroscience following the example of physics, we exemplified its ramifications in dendritic computation, cortical microcircuits, synaptic plasticity, motor control, and sensory-based decision-making. The crucial point of our axiomatization is that it connects the local neuronal errors to the global behavioral errors right in the formulation of the principle, eventually leading to local gradient-based plasticity rules. Because the formulation builds upon computations that can be realized in single neurons and dendrites to produce a behavioral output, the NLA principle can be seen as a bottom-up theory of behavior. It is articulated in terms of apical and basal dendrites, somatic firing, network connectivity and behavioral outputs that jointly minimize their errors. This contrasts the related free energy principle, for instance, that leads to a top-down theory of behavior by starting with the statistical, but the more universal, notion of a free energy. It postulates that any self-organizing system, that is at a statistical equilibrium with its environment, must minimize its free energy (Friston, 2010; Friston et al., 2022), and from there work down its way to neurons and dendrites (Bastos et al., 2012; Kiebel and Friston, 2011).
Starting with a single Lagrangian function that specifies the form of the somatodendritic prediction errors leaves some freedom for the interpretation and the implementation of the emerging dynamical equations for the voltages. We interpret errors to be represented in the apical dendrites of pyramidal neurons while sensory input targets the basal dendrites, but other dendritic configurations are conceivable (Mikulasch et al., 2023) that apply also to non-pyramidal neurons. We have chosen a specific interneuron circuitry to extract our apical errors, but other microcircuits or error representations might also be considered (Keller and Mrsic-Flogel, 2018). On the other hand, the derived gradient-based synaptic plasticity is tightly linked to the specific form of the somatodendritic prediction errors expressed in the Lagrangian and its interpretation, making specific predictions for synaptic plasticity (as outlined below). The â€˜externalâ€™ feedback entering through the cost function offers additional freedom to model behavioral interactions. We considered an explicit time course of a target voltage in motor neurons, for instance imposed by the feedback from muscle spindles that are themselves innervated by a prospective top-down signal to control muscle lengths (Papaioannou and Dimitriou, 2021; Dimitriou, 2022). But the cost may also link to reinforcement learning and express a delayed reward feedback delivered upon a behavioral decision of an agent acting in a changing environment (Friedrich et al., 2011; Friedrich and Senn, 2012).
A fundamental difficulty arises when the neuronal implementation of the Euler-Lagrange equations requires an additional microcircuit with its own dynamics. This is the case for the suggested microcircuit extracting the local errors. Formally, the representation of the apical feedback errors first needs to be learned before the errors can teach the feedforward synapses on the basal dendrites. We showed that this error learning can itself be formulated as minimizing an apical mismatch energy. What the lateral feedback through interneurons cannot explain away from the top-down feedback remains an apical prediction error. Ideally, while the network synapses targetting the basal tree are performing gradient descent on the global cost, the microcircuit synapses involved in the lateral feedback are performing gradient descent on local error functions, both at any moment in time. The simulations show that this intertwined system can in fact learn simultaneously with a common learning rate that is properly tuned. The cortical model network of inter- and pyramidal neurons learned to classify handwritten digits on the fly, with 10-digit samples presented per second. Yet, the overall learning is more robust if the error learning in the apical dendrites operates in phases without output teaching but with corresponding sensory activity, as may arise during sleep (see e.g., Deperrois et al., 2022; Deperrois et al., 2024).

The NLA principle integrates classical theories for cortical processing and learning
The prospective variational principle introduced with the NLA allows for integrating previous ideas on formalizing the processing and learning in cortical networks. Four such classical lines of theories come together. (i) The first line refers to the use of an energy function to jointly infer the neuronal dynamics and synaptic plasticity, originally formulated for discrete-time networks (Hopfield, 1982; Ackley et al., 1985), and recently extended to continuous-time networks (Scellier and Bengio, 2017). (ii) The second line refers to understanding error-backpropagation in the brain (Rumelhart et al., 1986; Xie and Seung, 2003; Whittington and Bogacz, 2017; Whittington and Bogacz, 2019; Lillicrap et al., 2020). (iii) The third line refers to dendritic computation and the use of dendritic compartmentalization for various functions such as nonlinear processing (Schiess et al., 2016; Poirazi and Papoutsi, 2020) and deep learning (Guerguiev et al., 2017; Sacramento et al., 2018; Haider et al., 2021). (iv) The fourth line refers to predictive coding (Rao and Ballard, 1999) and active inference (Pezzulo et al., 2022) to improve the sensory representation and motor output, respectively.

The NLA integrates and predicts features of synapses, dendrites, and circuits
Motivated by the predictive power of the least-action principle in physics, we ask about experimental confirmation and predictions of the NLA principle. Given its axiomatic approach, it appears astonishing to find various preliminary matches at the dendritic, somatic, interneuron, synaptic, and even behavioral levels. Some of these are:
More experimental and theoretical work is required to substantiate these links and test specific predictions, such as the apical error representation in cortical pyramidal neurons.
Overall, our approach adapts the least-action principle from physics to be applied to neuroscience, and couples it with a normative perspective on the prospective processing of neurons and synapses in global cortical networks and local microcircuits. Given its physical underpinnings, the approach may inspire the rebuilding of computational principles of cortical neurons and circuits in neuromorphic hardware (Bartolozzi et al., 2022). A step in this direction, building on the instantaneous computational capabilities by slowly integrating neurons, has made done by Haider et al., 2021. Given its aspiration for a theoretical framework in neurobiology, a next challenge would be to generalize the NLA principle to spiking neurons (Gerstner and Kistler, 2002; Brendel et al., 2020) with their potential for hardware implementation (Zenke and Ganguli, 2018; GÃ¶ltz et al., 2021; Cramer et al., 2022), to include attentional mechanisms in terms of dendritic gain modulation (Larkum et al., 2004) with a putative link to self-attention in artificial intelligence (Vaswani et al., 2017), to add second-order errors to cope with certainties (Granier et al., 2023), and to incorporate longer temporal processing as, for instance, offered by neuronal adaptation processes (La Camera et al., 2006) or realistically modelled dendrites (Chavlis and Poirazi, 2021).


METHODS
Euler-Lagrange equations as inverse low-pass filters
The theory is based on the look-ahead of neuronal quantities. In general, the look-ahead of a trajectory x(t) is defined via lookahead operator applied to x,
The lookahead operator is the inverse of the low-pass filter operator denoted by a bar,
This low-pass filtering can also be characterized by the differential equation Ï„xâ€¾Ë™(t)=âˆ’xâ€¾(t)+x(t), see Appendix 2. Hence, applying the low-pass filtering to x and then the lookahead operator (1+Ï„ddt) to xâ€¾(t), and using the Leibnitz rule for differentiating an integral, we calculate (1+Ï„ddt)xâ€¾(t)=x(t). In turn, applying first the lookahead, and then the low-pass filtering, also yields the original trace back, (1+Ï„ddt)x=xâ€¾+Ï„xâ€¾Ë™=x.
We consider an arbitrary network architecture with network neurons that are recurrently connected and that receive external input through an overall weight matrix ğ‘¾=(ğ‘¾in,ğ‘¾net), aggregated column-wise. The instantaneous presnyaptic firing rates are ğ’“=(ğ’“in,ğ’“net)T, interpreted as a single-column vector. A subset of network neurons are output neurons, OâŠ†N, for which target voltages ğ’–* may be imposed. Rates and voltages may change in time t. Network neurons are assigned a voltage ğ’–, generating the low-pass filtered rate ğ’“â€¾net=Ï(ğ’–), and a low-pass filtered error ğ’†â€¾=ğ’–âˆ’ğ‘¾ğ’“â€¾. We further define output errors eâ€¾o*=uo*âˆ’uo for oâˆˆO, and eâ€¾i*=0 for non-output neurons iâˆˆNâˆ–O. With this, the Lagrangian from Equation 3 takes the form
We next use that ğ’–=ğ’–~âˆ’Ï„ğ’–~Ë™, with the .~ operator defined in Equation 4, to write out the Lagrangian L in the canonical coordinates (ğ’–~,ğ’–~Ë™) as (see also Equation 3)
The neuronal dynamics is derived from requiring a stationary action (see Equation 5), which is generally solved by the Euler-Lagrange equations âˆ‚Lâˆ‚u~iâˆ’ddtâˆ‚Lâˆ‚u~Ë™i=0 (see Equation 6). Because u~ only arises in L in the compound u~âˆ’Ï„u~Ë™, the derivative of L with respect to u~ is identical to the derivative with respect to Ï„u~Ë™,
Using the lookahead operator Equation 14, the Euler-Lagrange equations can then be rewritten as
Since L(ğ’–~,ğ’–~Ë™)=L(ğ’–) and ğ’–=ğ’–~âˆ’Ï„ğ’–~Ë™, the derivative of L with respect to ğ’–~ is the same as the derivative of L with respect to ğ’–,
Plugging this into Equation 19, the Euler-Lagrange equations become a function of ğ’– and ğ’–Ë™,
Notice that, if we had directly calculated âˆ‚Lâˆ‚u~iâˆ’ddtâˆ‚Lâˆ‚u~Ë™i=0, the second-order time derivative u~Â¨i of the discounted future voltage would be absorbed in a first-order time derivative of the voltage. The reason is that u~Ë™iâˆ’Ï„u~Â¨i=uË™i, and u~Â¨i only arises in this combination because the Lagrangian L=L(ğ’–) is only a function of ğ’– and not of ğ’–Ë™. Hence, the acceleration term u~Â¨i disappears, while a voltage derivative uË™i appears.
The solution of this differential Equation 20 is âˆ‚Lâˆ‚ui=cieâˆ’tâˆ’t0Ï„, and hence any trajectory (u~i,u~Ë™i) which satisfy the Euler-Lagrange equations will hence cause âˆ‚Lâˆ‚ui to converge to zero with a characteristic time scale of Ï„. Since we require that the initialisation is at t0=âˆ’âˆ, we conclude that âˆ‚Lâˆ‚ui=0, as required in the rt-DeEP Theorem. For a table with all the mathematical abbreviations see Methods-Table 1.

Deriving the network dynamics from the Euler-Lagrange equations
We now derive the equations of motion from the Euler-Lagrange equations. Noticing that ğ’– enters in ğ’†â€¾=ğ’–âˆ’ğ‘¾ğ’“â€¾ twice, directly and through ğ’“â€¾net=Ï(ğ’–), and once in the output error ğ’†â€¾*, we calculate from 16, using ğ’“â€¾(ğ’–)=(ğ’“â€¾in,Ï(ğ’–))T and ğ‘¾=(ğ‘¾in,ğ‘¾net),
Remember that for non-output neurons i no target exists, and for those we set ğ’†â€¾i*=0. Next, we apply the lookahead operator to this expression, as required by the Euler-Lagrange Equation 19. In general (1+Ï„ddt)ğ’™â€¾=ğ’™â€¾+Ï„ğ’™â€¾Ë™=x, and we set for ğ’™â€¾ the expression on the right-hand side of Equation 21, ğ’™â€¾=ğ’†â€¾âˆ’ğâ€¾âˆ’Î²ğ’†â€¾*, which at the same time is ğ’™â€¾=âˆ‚Lâˆ‚ğ’–. Hence, the Euler-Lagrange equations in the form of Equation 20, (1+Ï„ddt)xâ€¾=0, translate into
To move from the middle to the last equality we replaced ğ’† with  ğ’†=(1+Ï„ddt)ğ’†â€¾=ğ’–+Ï„ğ’–Ë™âˆ’ğ‘¾ğ’“. In the last equality we interpret ğ’† as the sum of the two errors, ğ’†=ğ+Î²ğ’†*, again using the middle equality. This proves Equation 7a, Equation 7b.
Notice that the differential equation Ï„ğ’–Ë™=... in Equation 22 represents an implicit ordinary differential equation as on the right-hand side not only ğ’–, but also ğ’–Ë™ appears (in ğ’“ and ğ’†). The uniqueness of the solution ğ’–(t) for a given initial condition is only guaranteed if it can be converted into an explicit ordinary differential equation (see Sect. Appendix 3).
In taking the temporal derivative we assumed small learning rates such that terms including WË™ij can be neglected. The derived dynamics for the membrane potential of a neuron ui in Equation 22 show the usual leaky behavior of biological neurons. However, both presynaptic rates râ€¾i and prediction errors eâ€¾i enter the equation of motion with lookaheads, i.e., they are advanced (ri=râ€¾i+Ï„râ€¾Ë™i and ei=eâ€¾i+Ï„eâ€¾Ë™i), cancelling the low-pass filtering. Since râ€¾Ë™i=Ïâ€²(ui)uË™i, the rate and error, ri and ei, can also be seen as nonlinear extrapolations from the voltage and its derivative into the future.
The instantaneous transmission of information throughout the network at the level of the voltages can now be seen by low-pass filtering Equation 22 with initialization far back in the past,
with column vector ğ’“â€¾(ğ’–)=(ğ’“â€¾in,Ï(ğ’–))T and ğ’†â€¾=ğ’“â€¾netâ€²â‹…ğ‘¾netTğ’†â€¾+Î²ğ’†â€¾*. Hence, solving the voltage dynamics for ğ’– (Equation 7a), with apical voltage ğ’†=ğ’†â€¾+Ï„ğ’†â€¾Ë™ derived from Equation 7b, yields the somatic voltage ğ’– satisfying the self-consistency Equation 23 at any time. In other words, ğ’– and ğ’†â€¾â€˜propagate instantaneouslyâ€™.

Deriving the error backpropagation formula
For clarity, we derive the error backpropagation algorithm for layered networks here. These can be seen as a special case of a general network with membrane potentials ğ’– and all-to-all weight matrix ğ‘¾ (as introduced in Appendix 8), where the membrane potentials decompose into layerwise membrane potential vectors ğ’–l and the weight matrix into according to block diagonal matrices ğ‘¾l (with ğ‘¾l being the weights that project into layer l).
Assuming a network with N layers, by low-pass filtering the equations of motion we get
for all lâˆˆ1,..,N, with the output error ğ’†â€¾ğ’ of the general recurrent network becoming the error in the last layer, that itself is the target error, ğ’†â€¾ğ’=ğ’†â€¾N=Î²ğ’†â€¾*=Î²(ğ’–N*âˆ’ğ’–N). The error ğ’†â€¾=ğâ€¾+Î²ğ’†â€¾*, that we obtain from the general dynamics with ğâ€¾=ğ’“â€¾netâ€²â‹…ğ‘¾netTğ’†â€¾, see Equations 21 and 22, translates to an iterative formula for the error at the current layer l given the error at the downstream layer l+1, inherited from the drive ğ’“â€¾l=Ï(ğ’–l) of that downstream layer via ğ‘¾l+1,
and ğ’†â€¾N=Î²ğ’†â€¾* for the output layer. The learning rule that reduces ğ’†â€¾l by gradient descent is proportional to this error and the presynaptic rate, as stated by Theorem 1, is
for l=1...N. Equations 25 and 26 together take the form of the error backpropagation algorithm, where an output error is iteratively propagated through the network and used to adjust the weights in order to reduce the output cost C. From this, it is easy to see that without output nudging (i.e. Î²=0), the output error vanishes and consequently all other prediction errors vanish as well, ğ’†â€¾l=ğ’–lâˆ’ğ‘¾lğ’“â€¾l=0 for all lâ‰¤N. This also means that in the absence of nudging, no weight updates are performed by the plasticity rule.
The learning rule for arbitrary connectivities is obtained in the same way by dropping the layer-wise notation. In this case, low-pass filtering the equations of motion yields ğ’–=ğ‘¾ğ’“â€¾+ğ’†â€¾, as calculated in 23, and the low-pass filtered error ğ’†â€¾=ğâ€¾+Î²ğ’†â€¾*=ğ’“â€¾netâ€²â‹…ğ‘¾netTğ’†â€¾+Î²ğ’†â€¾*, as inferred from Equations 21 and 22. Hence, the plasticity rule in general reads

Proving theorem 1 (rt-DeEP)
The implicit assumption in Theorem 1 is that ğ’–Ë™ exists in the distributional sense for t&gt;âˆ’âˆ, which is the case for delta-functions in ğ’“in and step-functions in ğ’–*. Both parts (i) and (ii) of the Theorem are based on the requirement of stationary action Î´A=0, and hence on ğ’– satisfying the Euler-Lagrange equations in the form of Equation 22, (1+Ï„ddt)âˆ‚Lâˆ‚ğ’–=0. From the solution âˆ‚Lâˆ‚ui=ceâˆ’tâˆ’t0Ï„ we conclude that for initialization at t0=âˆ’âˆ we have âˆ‚Lâˆ‚ğ’–=0 for all t. It is the latter stronger condition that we require in the proof. With this, the main ingredient of the proof follows is the mathematical argument of Scellier and Bengio, 2017, according to which the total and partial derivative of L with respect to ğ‘¾ are identical, and this in our case is true for any time t,
For convenience we considered âˆ‚Lâˆ‚ğ’– to be a column vector, deviating from the standard notations (see tutorial end of sec:Integration). Analogously to Equation 28, we infer dLdÎ²=âˆ‚Lâˆ‚Î². Reading Equation 28 from the right to the left, we conclude that the learning rule ğ‘¾Ë™âˆâˆ’âˆ‚Lâˆ‚ğ‘¾=ğ’†â€¾ğ’“â€¾T for all Î²&gt;0 is gradient descent on L, i.e., ğ‘¾Ë™âˆâˆ’dLdğ‘¾. This total derivative of L can be analyzed for large and small Î².
(i) We show that in the limit of large Î², ğ‘¾Ë™ becomes gradient descent on the mismatch energy EM=12â€–ğ’†â€¾â€–2. For this we first show that there is a solution of the self-consistency equation ğ’–=ğ‘­(ğ’–)=ğ‘¾ğ’“â€¾+ğ’“â€¾netâ€²â‹…ğ‘¾netTğ’†â€¾+Î²ğ’†â€¾* that is uniformly bounded for all t and Î². For this we assume that the transfer function Ï(u) is non-negative, monotonically increasing, and bounded, that its derivative Ïâ€²(u) is bounded too, and that the input rates ğ’“in and the target potentials ğ’–ğ’* are also uniformly bounded. To show that under these conditions we always find a uniformly bounded solution ğ’–(t), we first consider the case where the output voltages are clamped to the target, ğ’–ğ’=ğ’–ğ’* such that ğ’†â€¾*=0. For simplicity, we assume that Ïâ€²(u)=0 for |u|â‰¥c0. For voltages ğ’– with ğ’–iâ‰¤c0 the recurrent input current ğ‘¾ğ’“â€¾ is bounded, say |(ğ‘¾ğ’“â€¾)j|â‰¤c1 for some c1&gt;c0. When including the error term ğ’“â€¾netâ€²â‹…ğ‘¾netTğ’†â€¾, the total current still remains uniformly bounded, say |ğ‘­(ğ’–)j|â‰¤c2 for all ğ’– with ğ’–iâ‰¤c0. Because for larger voltages ğ’–i&gt;c0 the error term vanishes due to a vanishing derivative Ïâ€²(ğ’–i)=0, the mapping ğ‘­(ğ’–) maps the c2-box ğ’– (for which |ğ’–i|â‰¤c2) onto itself. Brouwerâ€™s fixed point theorem then tells us that there is a fixed point ğ’–=ğ‘­(ğ’–) within the c2-box. The theorem requires the continuity of ğ‘­, and this is assured if the neuronal transfer function râ€¾=Ï(u) is continuous.
We next relax the voltages of the output neurons from their clamped stage, ğ’–ğ’=ğ’–ğ’*. Remember that these voltages satisfy ğ’–ğ’=(ğ‘¾ğ’“â€¾+ğ’“â€¾netâ€²â‹…ğ‘¾netTğ’†â€¾+Î²ğ’†â€¾*)ğ’=ğ‘­(ğ’–)ğ’ at any time t. We determine the correction term Î²ğ’†â€¾ğ’* such that in the limit Î²â†’âˆ we get ğ’–ğ’=ğ‘­(ğ’–)ğ’=ğ’–ğ’*. The correction remains finite, and in the limit must be equal to limÎ²â†’âˆâ¡Î²ğ’†â€¾ğ’*=ğ’–ğ’*âˆ’(ğ‘¾ğ’“â€¾+ğ’“â€¾netâ€²â‹…ğ‘¾netTğ’†â€¾)ğ’. For arbitrary large nudging strength Î², the output voltage ğ’–ğ’ deviates arbitrary little from the target voltage, ğ’–ğ’=ğ’–ğ’*+o(1/Î²), with target error ğ’†â€¾ğ’*=1Î²(ğ’–âˆ’ğ‘¾ğ’“â€¾âˆ’ğ’“â€¾netâ€²â‹…ğ‘¾netTğ’†â€¾)ğ’ shrinking like c2/Î². Likewise, also for non-output neurons i, the self-consistency solution ğ’–i=ğ‘­(ğ’–)i deviates arbitrarily little from the solution of the clamped state. To ensure the smooth drift of the fixed point while 1/Î² deviates from 0 we require that the Jacobian of ğ‘­ at the fixed point is invertible.
Because the output ğ’†â€¾ğ’* shrinks with 1/Î², the cost shrinks quadratically with increasing nudging strength, C=12â€–ğ’†â€¾*â€–2=o(1Î²2), and hence the cost term Î²2â€–ğ’†â€¾*â€–2 that enters in L=EM+Î²2â€–ğ’†â€¾*â€–2 vanishes in the limit Î²â†’âˆ. In this large Î² limit, where ğ’†â€¾ğ’*=0 and hence the outputs are clamped, ğ’–ğ’=ğ’–ğ’*, the Lagrangian reduces to the mismatch energy, L=EM. Along the least-action trajectories, we, therefore, get ğ‘¾Ë™âˆâˆ’âˆ‚Lâˆ‚ğ‘¾=âˆ’dLdğ‘¾=âˆ’dEMdğ‘¾. The first equality uses Equation 28, and the second uses L=EM just derived for Î²=âˆ. This is a statement (i) of Theorem 1. In the case of successful learning, EM=0, we also conclude that the cost vanishes, C=0. This is the case because EM=0 implies EMo=0 for all output neurons o. Since EMo=12ğ’†â€¾o2=12(ğ’“â€¾netâ€²â‹…ğ‘¾netTğ’†â€¾+Î²ğ’†â€¾*)o2, we conclude that ğ’†â€¾o=0, and if the output neurons do not feed back to the network (which we can assume without loss of generality), we conclude that ğ’†â€¾o*=0.
(ii) To consider the case of small Î², we use that the cost C can be expressed as C=âˆ‚Lâˆ‚Î². This is a direct consequence of how C enters in L=12â€–ğ’†â€¾â€–2+Î²2C, see Equation 16 and Scellier and Bengio, 2017. We now put this together with Equation 28 and the finding that âˆ‚Lâˆ‚Î²=dLdÎ². Since for the Lipschitz continuous function L in u, W, and Î² (L is even smooth in these arguments), the total derivatives interchange (which is a consequence of the Moore-Osgood theorem applied to the limits of the difference quotients), we then get at any t,
The last expression is calculated from the specific form of the Lagrangian Equation 17, using that by definition ğ’†â€¾=ğ’–âˆ’ğ‘¾ğ’“â€¾.
Finally, in the absence of output nudging, Î²=0, we can assume vanishing errors, ğ’†â€¾=0, as they solve the self-consistency equation, ğ’†â€¾=ğ’“â€¾netâ€²â‹…ğ‘¾netTğ’†â€¾ for all t, see Equation 27. For these solutions we have ğ’†â€¾ğ’“â€¾T|Î²=0=0. Writing out the total derivative of the function ğ’ˆ(Î²)=ğ’†â€¾ğ’“â€¾T with respect to Î² at Î²=0 as limit of the difference quotient, dğ’ˆ(Î²)dÎ²|Î²=0=limÎ²â†’0â¡1Î²(ğ’ˆ(Î²)âˆ’ğ’ˆ(0))=limÎ²â†’0â¡1Î²ğ’ˆ(Î²), using that ğ’ˆ(0)=0, we calculate at any t,
Here, we assume that ğ’†â€¾ğ’“â€¾T is evaluated at Î²&gt;0 (that itself approaches 0), while ğ’†â€¾ğ’“â€¾T|Î²=0 is evaluated at Î²=0. Combining Equations 29 and 30 yields the cost gradient at any t,
This justifies the gradient learning rule ğ‘¾Ë™ in Equation 27. Learning is stochastic gradient descent on the expected cost, where stochasticity enters in the randomization of the stimulus and target sequences ğ’“in(t) and ğ’–*(t). For the regularity statement, see â€˜From implicit to explicit differential equationsâ€™ in the sec:Integration. Notice that this proof works for a very general form of the Lagrangian L, until the specific expression for âˆ‚Lâˆ‚ğ‘¾. For a proof in terms of partial derivatives only, see Appendix 8, and for a primer on partial and total derivatives see Appendix 7.

Instantaneous gradient descent on C(ğ’–ğ’*,ğ’“in)
The cost C=12â€–ğ’–ğ’*âˆ’ğ’–ğ’â€–2 at each time t is a function of the voltage ğ’–ğ’ of the output neurons and the corresponding targets. In a feedforward network, due to the instantaneity of the voltage propagation Equation 23, ğ’–ğ’ is in the absence of output nudging (Î²=0) an instantaneous function of the voltage at the first layer, ğ’–1(t)=ğ‘¾inğ’“in(t)+ğ’–1(t0)eâˆ’tâˆ’t0Ï„. For initialisation at t0=âˆ’âˆ, the second term vanishes for all t and hence ğ’–1(t)=ğ‘¾inğ’“in(t). The output voltage ğ’–ğ’(t), therefore, becomes a function ğ‘­W of the low-pass filtered input rate ğ’“in(t) that captures the instantaneous network mapping, ğ’–ğ’(t)=ğ‘­W(ğ’“in(t)), and with this the cost also becomes an instantaneous function of ğ’“in and ğ’–ğ’*, namely C(t)=12â€–ğ’–ğ’*(t)âˆ’ğ’–ğ’(t)â€–2=12â€–ğ’–ğ’*(t)âˆ’ğ‘­W(ğ’“in(t))â€–2.
For a general network, again assuming t0=âˆ’âˆ, the voltage is determined by the vanishing gradient âˆ‚Lâˆ‚ğ’–=ğ’‡(ğ’–,t)=ğ’–âˆ’ğ‘¾ğ’“â€¾(ğ’–)âˆ’ğ’†â€¾(ğ’–)=0 with ğ’†â€¾=ğâ€¾âˆ’Î²ğ’†â€¾*, see Equation 21. For the inclusive treatment of the initial transient see Appendix 3 and Appendix 4. Remember that ğ’“â€¾=(ğ’“â€¾in,ğ’“â€¾net(ğ’–))T and ğ’†â€¾*=ğ’–ğ’*âˆ’ğ’–ğ’. For a given ğ’“â€¾in and ğ’–ğ’* at time t, the equation ğ’‡(ğ’–,t)=0 can be locally solved for ğ’– if the Hessian ğ‘¯=âˆ‚2Lâˆ‚ğ’–2=âˆ‚ğ’‡âˆ‚ğ’–=1âˆ’ğ‘¾netğ†â€²âˆ’ğ’†â€¾â€² is invertible, ğ’–=ğ‘­(ğ’“â€¾in,ğ’–ğ’*). This mapping can be restricted to the output voltages ğ’–ğ’ on the left-hand side, while replacing ğ’–ğ’*=ğ’–ğ’+ğ’†â€¾ğ’* in the argument on the right-hand side (even if this again introduces ğ’–ğ’ there). With this, we obtain the instantaneous mapping ğ’–ğ’(t)=ğ‘­W(ğ’“in(t),ğ’†â€¾ğ’*(t)) from the low-pass filtered input and the output error to the output itself. Notice that for functional feedforward network, the network weight matrix ğ‘¾net is lower triangular, and for small enough Î² the Hessian ğ‘¯ is, therefore, always positive definite (see also Methods, Sect. From implicit to explicit differential equations).

Proving theorem 2 (rt-DeEL)
Here, we restrict ourselves to layered network architectures. To prove Theorem 2 first assume that interneurons receive no nudging (Î²I=0) and only the lateral interneuron-to-pyramidal weights ğ‘¾PIl are plastic. This is already sufficient to prove the rt-DeEL theorem. Yet, simulations showed that learning the lateral pyramidal-to-interneuron weights ğ‘¾IPl via top-down nudging, so that the interneuron activity mimics the upper layer pyramidal neuron activity, helps in learning a correct error representation. We consider this case of learning ğ‘¾IPl later.
If the microcircuits is ought to correctly implement error backpropagation, all local prediction errors ğ’†â€¾l must vanish in the absence of output nudging (Î²=0) as there is no target error. Consequently, any remaining errors in the network are caused by a misalignment of the lateral microcircuit. We show how learning the interneuron-to-pyramidal weights ğ‘¾PIl corrects for such misalignments.
To define the gradient descent plasticity of the weights WPIl from the interneurons to the pyramidal neurons, we consider the apical error formed by the difference of top-down input and interneuron input, ğ’†â€¾lA=ğ‘©lğ’–l+1âˆ’ğ‘¾PIlğ’–Il, and define the apical mismatch energy as ElPI=12â€–ğ’†â€¾lAâ€–2. Gradient descent along this energy with respect to WPIl yields
evaluated online while presenting input patterns from the data distribution to the network. We assume that the apical contribution to the somatic voltage is further modulated by the somatic spike rate, ğ’“â€¾lâ€²â‹…ğ’†â€¾lA. After successful learning, the top-down input ğ‘©lğ’–l+1 is fully subtracted away by the lateral input in the apical compartment, and we have
Once this condition is reached, the network achieves a state where, over the activity space spanned by the data, top-down prediction errors throughout the network vanish,
We show that this top-down prediction error, after the successful learning of the microcircuit, shares the properties of error-backpropagation for a suitable backprojection weights ğ‘©.
Due to the vanishing prediction errors, pyramidal cells only receive bottom-up input ğ’–l+1=ğ‘¾l+1ğ’“â€¾l. Using this expression as well as the expression for interneuron membrane potentials without top-down nudging (Î²I=0 in Equation 9), ğ’–Il=ğ‘¾IPlğ’“â€¾l, and plugging both into Equation 33, we get
Assuming that ğ‘¾IPl has full rank, and the low-pass filtered rates ğ’“â€¾l span the full nl dimensions of layer l when sampled across the data set, we conclude that
In other words, the loop via upper layer and back is learned to be matched by a lateral loop through the interneurons.
Equation 36 imposes a restriction on the minimal number of interneurons nlI at layer l. In fact, the matrix product ğ‘©lğ‘¾l+1 maps a nl-dimensional space onto itself via nl+1-dimensional space. The maximal rank of the this matrix product is limited by the smallest dimension, i.e., rank(ğ‘©lğ‘¾l+1)â‰¤min(nl,nl+1). Analogously, rank(ğ‘¾PIlğ‘¾IPl)â‰¤min(nl,nlI). But since the two ranks are the same according to Equation 36, we conclude that in general nlIâ‰¥min(nl,nl+1) must hold, i.e., there should be at least as many interneurons at layer l as the lowest number of pyramidal neurons at either layer l or l+1. Note that by choosing nlI=nl+1 as in Sacramento et al., 2018 (or nlI&gt;nl+1 as in this work), the conditions is fulfilled.
With ğ’–Il=ğ‘¾IPlğ’“â€¾l and Equation 36, the top-down prediction error from Equation 34, in the presence of output nudging (Î²&gt;0), can be written in the backpropagation form
Finally, the simulations showed that learning the lateral weights in the microcircuit greatly benefits from also adapting the pyramidal-to-interneuron weights ğ‘¾IP by gradient descent on EIP=12âˆ‘lâ€–ğ’–Ilâˆ’ğ‘¾IPlğ’“â€¾lâ€–2, using top-down nudging of the inhibitory neurons (Î²I&gt;0),
After learning we have ğ’–Il=ğ‘¾IPlğ’“â€¾l, and plugging in ğ’–Il=(1âˆ’Î²I)ğ‘¾IPlğ’“â€¾l+Î²Iğ‘©IPlğ’–l+1 (Equation 9), we obtain ğ‘¾IPlğ’“â€¾l=ğ‘©IPlğ’–l+1. Since ğ’–l+1=ğ‘¾l+1ğ’“â€¾l, we conclude as before,
The top-down weights BIPl that nudge the lower-layer interneurons has randomized entries and may be considered as full rank. If there are less pyramidal neurons in the upper layer than interneurons in the lower layer, BIPl selects a subspace in the interneuron space of dimension nl+1&lt;nlI. This seems to simplify the learning of the interneuron-to-pyramidal cell connections WPI. In fact, this learning now has only to match the nl+1-dimensional interneuron subspace embedded in nlI dimensions to an equal (nl+1-)dimensional pyramidal cell subspace emedded in nl dimensions.
Learning of the interneuron-to-pyramidal cell connections works with the interneuron nudging as before, and combining Equations 36 with 39 yields the â€˜loop consistencyâ€™
The learning of the microcircuit was described in the absence of output nudging. Conceptually, this is not a problem as one could introduce a pre-learning phase where the lateral connections are first correctly aligned before learning of the feedforward weights begins. In simulations we find that both the lateral connections as well as the forward connections can be trained simultaneously, without the need for such a pre-learning phase. We conjecture that this is due to the fact that our plasticity rules are gradient descent on the energy functions L, EPI, and EIP, respectively.

From implicit to explicit differential equations
The voltage dynamics is solved by a forward-Euler scheme ğ’–(t+dt)=ğ’–(t)+ğ’–Ë™(t)dt. The derivative ğ’–Ë™(t) is calculated either through (i) the implicit differential Equation 7a yielding Ï„ğ’–Ë™(t)=ğ’‰(ğ’–(t),ğ’–Ë™(tâˆ’dt)), or (ii) by isolating ğ’–Ë™(t) and solving for the explicit differential equation Ï„ğ’–Ë™(t)=ğ’ˆ(ğ’–(t)), as explained in Appendix 3 (after Equation 51).
(i) The implicit differential equation, Ï„ğ’–Ë™(t)=âˆ’ğ’–(t)+ğ‘¾ğ’“(t)+ğ’†(t), see Equation 22, is iteratively solved by assigning ğ’“(t)=Ï(ğ’–(t))+Ïâ€²(ğ’–(t))â‹…ğ’–Ë™(tâˆ’dt) and calculating the error ğ’†(t)=ğ’†â€¾(t)+Ï„ğ’†â€¾Ë™(t) with ğ’†â€¾(ğ’–)=Ïâ€²(ğ’–)â‹…ğ‘¾netT(ğ’–âˆ’ğ‘¾netÏ(ğ’–)âˆ’ğ‘¾inğ’“â€¾in)+Î²ğ’†â€¾* and ğ’†â€¾Ë™(t)=ğ’†â€¾â€²(ğ’–(t))â‹…ğ’–Ë™(tâˆ’dt).
This iteration exponentially converges to a fixed point uË™(t) on a time scale dt1âˆ’k, where 1âˆ’k&gt;0 is the smallest Eigenvalue of the Hessian ğ‘¯=âˆ‚2Lâˆ‚ğ’–2=1âˆ’ğ‘¾netğ†â€²âˆ’ğ’†â€¾â€², see Appendix 3.
(ii) The explicit differential equation is obtained by eliminating the ğ’–Ë™ from the right-hand side of the implicit differential equation. Since ğ’–Ë™ enters linearly we get Ï„ğ‘¯ğ’–Ë™=âˆ’ğ’‡âˆ’Ï„âˆ‚ğ’‡âˆ‚t with ğ’‡(ğ’–,t)=âˆ‚Lâˆ‚ğ’–=ğ’–âˆ’ğ‘¾ğ’“â€¾âˆ’ğâ€¾âˆ’Î²ğ’†â€¾*. The explicit form is obtained by matrix inversion, ğ’–Ë™=ğ’ˆ(ğ’–,t)=âˆ’1Ï„ğ‘¯âˆ’1(ğ’‡+Ï„âˆ‚ğ’‡âˆ‚t), as the Hessian is invertible if it is strictly positive definite (which is typically the case, see Appendix 3, after Equation 48). The external input and the target enter through âˆ‚ğ’‡âˆ‚t=ğ‘¾inğ’“â€¾Ë™in+Î²ğ’–Ë™ğ’*, where the derivative of the target voltage is only added for the output neurons ğ’. This explicit differential equation is shown to be contractive in the sense that for each input trajectory ğ’“in(t) and target trajectory ğ’–*(t), the voltage trajectory ğ’–(t) is locally attracting for neighbouring trajectories. This local attracting trajectory is the vanishing-gradient trajectory ğ’‡(ğ’–,t)=0, and the gradient remains 0 even if the input contains delta-functions, see Appendix 4.
Moving and latent equilibria: a formal definition
We showed that the motor output (ğ’–ğ’), together with the low-pass filtered sensory input (ğ’“â€¾in) and the motor feedback (ğ’†â€¾ğ’*) is in a moving equilibrium, ğ’–ğ’=ğ‘­W(ğ’“â€¾in,ğ’†â€¾ğ’*), see Figure 3a. In general, a dynamical system in ğ’– that is given in an implicit form ğ‘®(ğ’™,ğ’™Ë™,ğ’–,ğ’–Ë™)=0 with external inputs (ğ’™,ğ’™Ë™) is said to be in a moving equilibrium if the variable ğ’– is an instantaneous function of the input ğ’™ at any point in time, ğ’–=ğ‘­(ğ’™). The fact that the implicit differential equation ğ‘®=0 represents a dynamical system in ğ’– implies that, in principle, it has a representation in the explicit form ğ’–Ë™=ğ’ˆ(ğ’–,ğ’™,ğ’™Ë™), guaranteed by an invertible Jacobian âˆ‚ğ‘®âˆ‚ğ’–Ë™.
Our example is obtained from ğ‘®=(1+ğ‰â‹…ddt)ğ’‡ with ğ’‡(ğ’–,ğ’™)=âˆ‚Lâˆ‚ğ’– and ğ’™=(ğ’“â€¾in,ğ’†â€¾ğ’*), leading to ğ’™+ğ‰â‹…ğ’™Ë™=(ğ’“in,ğ’†ğ’*). Given the paramterization of ğ‘® by the weights, we get the parametrized function ğ’–=ğ‘­W(ğ’™), and this is restricted to the output components ğ’–ğ’ of ğ’–. The condition on the Jacobian translates to âˆ‚ğ‘®âˆ‚ğ’–Ë™=âˆ‚ğ’‡âˆ‚ğ’–=âˆ‚2Lâˆ‚ğ’–2 being invertible. Crucially, the description of the dynamics in the biological or physical substrate is not given in its explicit form ğ’–Ë™=ğ’ˆ(ğ’–,ğ’™,ğ’™Ë™). However, it is given in an implicit form expressed as ğ’–Ë™=ğ’‰(ğ’™,ğ’™Ë™,ğ’–,ğ’–Ë™), where ğ’–Ë™ still appears on the right-hand side. This â€˜hybridâ€™ form is directly solved either in real time by the biophysical substrate itself, or by the forward-Euler scheme on clocked hardware, see (i) above. Notice that moving equilibria ğ’–=ğ‘­W(ğ’™) with ğ’™=(ğ’“â€¾in,ğ’†â€¾ğ’*) are able to capture complex temporal processing of the instantaneous input ğ’“in. In fact, the low-pass filtering ğ’“â€¾in can be obtained on various time scales through different Ï„inâ€™s, and ğ‘­W for a general network ğ‘¾ can be arbitrary complex. The task is to adapt ğ‘¾ such that the â€˜hybridâ€™ dynamical system eventually implements the target mapping ğ’–ğ’*=ğ‘­*(ğ’™).
The Latent Equilibrium (Haider et al., 2021) can be analogously formalized as a dynamical system in ğ’–, implicitly given by ğ‘®(ğ’™,ğ’–,ğ’–Ë™)=0, and having a solution of the form ğ’–+ğ‰â‹…ğ’–Ë™=ğ‘­(ğ’™). Abbreviating again ğ’‡(ğ’–,ğ’™)=âˆ‚Lâˆ‚ğ’– with the same Lagrangian L=12â€–ğ’–âˆ’ğ‘¾Ï(ğ’–)â€–2+Î²2C as in the present NLA, the Latent Equilibrium is obtained for ğ‘®(ğ’™,ğ’–,ğ’–Ë™)=ğ’‡(ğ’–+ğ‰â‹…ğ’–Ë™,ğ’™). The solution implies that the rate ğ’“=Ï(ğ’–+ğ‰â‹…ğ’–Ë™)=Ï(ğ‘­(ğ’™)) is an instantaneous function of ğ’™=(ğ’“in,ğ’†ğ’*), here without low-pass filtering. As for moving equilibria, the crucial point is that the biophysical substrate implements a hybrid form of the dynamical system, now ğ’–Ë™=ğ’‰(ğ’™,ğ’–,ğ’–Ë™), that is implicitly solved by the analog substrate, and also allows for a solution in clocked hardware. For an extended stability analysis of the moving and latent equilibria see Appendix 4.


Simulation details
Solving the explicit differential equation seems to be more robust when the learning rate for ğ‘¾Ë™ gets larger. The explicit form is also less sensitive to large Euler steps dt, see Appendix 3. By this reason, the ordinary differential equations (ODE) were solved in the explicit form when including plasticity ğ‘¾Ë™. The algorithms are summarized as follows, once without interneurons (Algorithm 1), and once with interneurons (Algorithm 2):
: 
: 
Details for Figure 3b
Color coded snapshot of cortical local field potentials (LFPs) in a human brain from 56 deep iEEG electrodes at various locations, converted with the sigmoidal voltage-to-rate function râ€¾(u)=11+eâˆ’u and plotted onto a standard Talairach Brain (Talairach and Tournoux, 1988). The iEEG data is from a patient with pharmacoresistant epilepsy and electrodes implanted during presurgical evaluation, extracted from the data release of Burrello et al., 2019. The locations of the electrodes are chosen in accordance with plausibilty, as the original positions of the electrodes were omitted due to ethical standards to prevent patient identification.

Details for Figure 3c
Simulations of the voltage dynamics (Equation 7a) and weight dynamics (Equation 8), with learning rate Î·=10âˆ’3, step size dt= 1ms for the forward Euler integration, membrane time constant  Ï„=10ms and logistic activation function. Weights were initialized randomly from a normal distribution N(0,0.12) with a cut-off at Â± 0.3. The number of neurons in the network N was n=96, among them 56 output neurons OâŠ‚N that were simultaneously nudged, and 40 hidden neurons. During training, all output neurons were nudged simultaneously (with Î²=0.1), whereas during testing, only 42 out of 56 neurons were nudged, the remaining 14 left to reproduce the traces. Data points of the iEEG signal were sampled with a frequency of 512 Hz. For simplicity, we, therefore, assumed that successive data points are separated by 2ms, and up-sampled the signal via simple interpolation to 1 ms resolution as required by our integration scheme. Furthermore, the raw values were normalized by dividing them by a factor of 200 to ensure that they are approximately in a range of Â±1â€“2. Training and testing was done on two separate 8 s traces of the iEEG recording. Same data as in Figure 3b1.

Details for Figure 4
Simulation of the neuronal and synaptic dynamics as given by Equation 8, Equation 7a, Equation 7b. For 5 ms, 10 ms, and 50 ms presentation time, we used an integration step size of dt=0.05ms, dt=0.1ms and dt=0.5ms, respectively (and dt=1ms otherwise). As an activation function, we used the step-linear function (hard sigmoidal) with râ€¾(u)=0 for uâ‰¤0, râ€¾(u)=1 for uâ‰¥1 and râ€¾(u)=u in between. The learning rate was initially set to Î·=10âˆ’3 and then reduced to Î·=10âˆ’4 after 22,000 s. The nudging strength was Î²=0.1 and the membrane time constant Ï„=10ms. In these simulations (and only for these) we assumed that at each presynaptic layer l=0,1,..,nâˆ’1 there is a first neuron indexed by 0 that fires with constant rate râ€¾l,0=1, effectively allowing the postsynaptic neurons ğ’“â€¾l+1 to learn a bias through the first column of the weight matrix ğ‘¾l+1. Weights were initialized randomly from a normal distribution N(0,0.012) with a cut-off at Â±0.03. For an algorithmic conversion see the scheme below. In Figure 4c1, â€˜rt-DeEP w/o lookaheadâ€™ is based on the dynamics Ï„ğ’–Ë™=âˆ’ğ’–+ğ‘¾ğ’“â€¾+ğ’†â€¾. For â€˜ğ’–Ë™ w/o error + backprop,â€™ we use Ï„ğ’–Ë™=âˆ’ğ’–+ğ‘¾ğ’“ as the forward model (so without error terms on the membrane potential, but a prospective ğ’“), and calculate weight updates using error backpropagation. In 4c2, we provide three controls: the test error for (i) a standard shallow artificial neural network trained on MNIST (black dashed line), (ii) rt-DeEP without prospective coding (as in Figure 4c1), but in Figure 4c2 with plasticity only turned on when the network is completely stationary, i.e., after waiting for several 100ms, such that synaptic weights are not changed during transients (orange dashed line, denoted by â€˜w/o transientsâ€™), and (iii) an equivalent artificial neural network, ğ’–l=ğ‘¾lğ’“â€¾lâˆ’1, trained using error backpropagation (black dashed line, â€˜standard backpropâ€™).

Details for Figure 5
Simulation of neuronal and synaptic dynamics with plastic microcircuit, i.e., the pyramidal-to-interneuron and lateral weights of the microcircuit learned during training.
For the results shown in Figure 5c2, the following parameters were used. As an activation function, we used a hard sigmoid function and the membrane time constant was set to Ï„=10 ms. Image presentation time is 100ms. Forward, pyramidal-to-interneuron and interneuron-to-pyramidal weights were initialized randomly from a normal distribution N(0,0.012) with a cut-off at Â±0.03. All learning rates were chosen equal Î·=10âˆ’3 and were subsequently reduced to Î·=10âˆ’4 after 22,000 s training time. The nudging parameters were set to Î²=0.1 and Î²I=0.11.1. The feedback connections ğ‘©l and the nudging matrices ğ‘©lIP were initialized randomly from a normal distribution 5â‹…N(0,0.012) with a cut-off at Â±0.15. The used integration step size was dt=0.25 ms. All weights were trained simultaneously. For an algorithmic conversion see the scheme below. The interneuron membrane potential was calculated by Equation 9 with a linear transfer function.

